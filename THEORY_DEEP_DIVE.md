# ğŸ“ ç†è®ºæ·±åº¦å­¦ä¹ æŒ‡å—ï¼šåˆ†å¸ƒå¼ç³»ç»Ÿä¸å¤§æ•°æ®å¤„ç†æ ¸å¿ƒç†è®º

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**: å»ºç«‹æ‰å®çš„ç†è®ºåŸºç¡€ï¼ŒåŸ¹å…»å­¦æœ¯ç ”ç©¶æ€ç»´ï¼ŒæŒæ¡åˆ†å¸ƒå¼ç³»ç»Ÿä¸å¤§æ•°æ®å¤„ç†çš„æ ¸å¿ƒåŸç†
> 
> ğŸ“š **é€‚ç”¨äººç¾¤**: å¸Œæœ›æ·±å…¥ç†è§£åˆ†å¸ƒå¼ç³»ç»Ÿç†è®ºçš„ç ”ç©¶è€…ã€å·¥ç¨‹å¸ˆã€å­¦è€…
> 
> â±ï¸ **å»ºè®®å­¦ä¹ æ—¶é—´**: 4-8å‘¨ï¼ˆæ ¹æ®èƒŒæ™¯è°ƒæ•´ï¼‰

---

## ğŸ“– å­¦ä¹ å¯¼èˆª

### ğŸ¯ æ ¸å¿ƒæ¨¡å—
| æ¨¡å— | ç†è®ºæ·±åº¦ | å®è·µéš¾åº¦ | å­¦ä¹ å‘¨æœŸ | å…³é”®è®ºæ–‡ |
|------|----------|----------|----------|----------|
| **Actoræ¨¡å‹ç†è®º** | â­â­â­â­ | â­â­â­ | 1-2å‘¨ | Hewitt(1973), Agha(1986) |
| **åˆ†å¸ƒå¼ä¸€è‡´æ€§** | â­â­â­â­â­ | â­â­â­â­ | 2-3å‘¨ | Brewer(2000), Lamport(1990) |
| **æŸ¥è¯¢ä¼˜åŒ–ç†è®º** | â­â­â­â­ | â­â­â­â­ | 2-3å‘¨ | Graefe(1994), Selinger(1979) |
| **å†…å­˜è®¡ç®—åŸç†** | â­â­â­ | â­â­â­â­â­ | 1-2å‘¨ | Stonebraker(2005) |

### ğŸ› ï¸ å­¦ä¹ å·¥å…·ç®±
- **è®ºæ–‡é˜…è¯»**: Zotero + PDFæ ‡æ³¨å·¥å…·
- **ç†è®ºéªŒè¯**: Scala/Java + æ•°å­¦è¯æ˜å·¥å…·
- **å®éªŒç¯å¢ƒ**: Docker + Kubernetesé›†ç¾¤
- **æ€§èƒ½åˆ†æ**: perf + JProfiler + è‡ªå®šä¹‰åŸºå‡†

---

## ğŸ—ï¸ 1. åˆ†å¸ƒå¼ç³»ç»Ÿç†è®º

### ğŸ“– 1.1 Actoræ¨¡å‹ç†è®ºä½“ç³»

#### ğŸ›ï¸ å¥ åŸºæ€§è®ºæ–‡ç²¾è¯»

**"A Universal Modular Actor Formalism for Artificial Intelligence" (1973)**
- **ä½œè€…**: Carl Hewitt, Peter Bishop, Richard Steiger
- **å‘è¡¨**: IJCAI 1973
- **å¼•ç”¨**: ~5000+ å¼•ç”¨ï¼ŒActoræ¨¡å‹å¥ åŸºä¹‹ä½œ
- **æ ¸å¿ƒè´¡çŒ®**: 
  - å®šä¹‰äº†Actorä½œä¸ºå¹¶å‘è®¡ç®—çš„åŸºæœ¬å•å…ƒ
  - æå‡ºäº†æ¶ˆæ¯ä¼ é€’çš„å¼‚æ­¥é€šä¿¡æ¨¡å‹
  - å»ºç«‹äº†æ¨¡å—åŒ–ç³»ç»Ÿçš„æ•°å­¦åŸºç¡€
  - å¼•å…¥äº†"become"è¯­ä¹‰è¿›è¡Œè¡Œä¸ºåˆ‡æ¢

**è®ºæ–‡æ ¸å¿ƒæ€æƒ³è§£æ**:
> "An actor is a computational entity that, in response to a message it receives, can concurrently:
> 1. Send a finite number of messages to other actors
> 2. Create a finite number of new actors
> 3. Designate the behavior to be used for the next message it receives"

**æ•°å­¦å½¢å¼åŒ–å®šä¹‰**:
```scala
// Actoræ¨¡å‹çš„æ•°å­¦å®šä¹‰ (åŸºäºÎ»æ¼”ç®—æ‰©å±•)
type Actor = (Address, Behavior, State)
type Behavior = Message Ã— State â†’ Action Ã— State'
type Action = Send[Message, Address] | Create[Actor] | Become[Behavior]

// æ¶ˆæ¯ä¼ é€’çš„è¯­ä¹‰è§„åˆ™
âŸ¨aáµ¢, báµ¢, sáµ¢âŸ© + m â†’ (send(aâ±¼, mâ±¼), create(aâ‚–), become(b'))
```

**ç†è®ºè¦ç‚¹æ·±åº¦åˆ†æ**:
1. **å°è£…æ€§ (Encapsulation)**: 
   - Actorå†…éƒ¨çŠ¶æ€åªèƒ½é€šè¿‡æ¶ˆæ¯é—´æ¥è®¿é—®
   - é¿å…äº†å…±äº«çŠ¶æ€çš„å¹¶å‘é—®é¢˜
   - æ•°å­¦ä¿è¯: çŠ¶æ€è½¬æ¢å‡½æ•°æ˜¯çº¯å‡½æ•°

2. **å¼‚æ­¥é€šä¿¡ (Asynchronous Communication)**:
   - æ¶ˆæ¯å‘é€æ˜¯éé˜»å¡æ“ä½œ
   - æ¶ˆæ¯ä¼ é€’éµå¾ªFIFOè¯­ä¹‰ï¼ˆåœ¨åŒä¸€é€šé“ä¸Šï¼‰
   - æ—¶é—´è§£è€¦ï¼šå‘é€è€…å’Œæ¥æ”¶è€…ä¸éœ€è¦åŒæ—¶å­˜åœ¨

3. **å¹¶å‘æ€§ (Concurrency)**:
   - å¤šä¸ªActorå¯å¹¶è¡Œæ‰§è¡Œ
   - æ— é”å¹¶å‘ï¼šé¿å…äº†ä¼ ç»Ÿå¹¶å‘ç¼–ç¨‹ä¸­çš„æ­»é”é—®é¢˜
   - ç†è®ºä¿è¯ï¼šActorç³»ç»Ÿæ˜¯ç¡®å®šæ€§å¹¶å‘æ¨¡å‹

4. **ä½ç½®é€æ˜ (Location Transparency)**:
   - Actoråœ°å€ä¸ç‰©ç†ä½ç½®è§£è€¦
   - æœ¬åœ°å’Œè¿œç¨‹æ¶ˆæ¯ä¼ é€’è¯­ä¹‰ä¸€è‡´
   - æ”¯æŒåŠ¨æ€è¿ç§»å’Œè´Ÿè½½å‡è¡¡

#### ğŸ“š æ‰©å±•é˜…è¯»ï¼šActoræ¨¡å‹çš„æ•°å­¦åŸºç¡€

**"Actors: A Model of Concurrent Computation in Distributed Systems" (Gul Agha, 1986)**
- **è´¡çŒ®**: å°†Actoræ¨¡å‹å»ºç«‹åœ¨ä¸¥æ ¼çš„æ•°å­¦åŸºç¡€ä¹‹ä¸Š
- **ç†è®º**: ä½¿ç”¨åŸŸç†è®º(Domain Theory)å’ŒæŒ‡ç§°è¯­ä¹‰
- **å…³é”®æ¦‚å¿µ**: 
  - **Actoré…ç½®**: Actoré›†åˆçš„æ•°å­¦æè¿°
  - **äº‹ä»¶ç»“æ„**: æè¿°å¹¶å‘äº‹ä»¶çš„ååºå…³ç³»
  - **æ”¶æ•›æ€§**: Actorè®¡ç®—çš„ç»ˆæ­¢æ€§åˆ†æ

**æ•°å­¦åŸºç¡€æ‰©å±•**:
```scala
// åŸºäºåŸŸç†è®ºçš„Actorè¯­ä¹‰
domain ActorConfig = P_finite(Actor)  // æœ‰é™Actoré›†åˆçš„å¹‚é›†
domain Event = (Actor, Message, Time)  // äº‹ä»¶ä¸‰å…ƒç»„
domain Computation = Event â†’ Event â†’ ...  // äº‹ä»¶åºåˆ—

// æ”¶æ•›æ€§å®šç†
Theorem: å¯¹äºæœ‰ç•ŒActorç³»ç»Ÿï¼Œè®¡ç®—åºåˆ—å¿…ç„¶æ”¶æ•›åˆ°å›ºå®šç‚¹
```

#### ğŸŒŸ ç°ä»£åˆ†å¸ƒå¼Actorç³»ç»Ÿæ¼”è¿›

**ä»ç†è®ºåˆ°å®è·µçš„æ¼”è¿›è·¯å¾„**:
```
ç†è®ºå‘å±•è·¯å¾„:
Hewitt Actor Model (1973)  â† åŸºç¡€ç†è®º
    â†“
Agha Actor Semantics (1986)  â† æ•°å­¦å½¢å¼åŒ–
    â†“
Clinger Actor Theory (1981)  â† å¹¶å‘è¯­ä¹‰
    â†“

å®è·µå‘å±•è·¯å¾„:
Erlang/OTP (1986)  â† ç¬¬ä¸€ä¸ªå®ç”¨çš„Actorç³»ç»Ÿ
    â†“ (ç”µä¿¡çº§å¯é æ€§è¦æ±‚)
Scala Actors (2006)  â† JVMä¸Šçš„ç±»å‹åŒ–Actor
    â†“ (å‡½æ•°å¼ç¼–ç¨‹èåˆ)
Akka Framework (2006)  â† ä¼ä¸šçº§Actoræ¡†æ¶
    â†“ (å¤§è§„æ¨¡åˆ†å¸ƒå¼è¦æ±‚)
Apache Pekko (2022)  â† Akkaçš„å¼€æºå»¶ç»­
    â†“ (ç¤¾åŒºé©±åŠ¨å‘å±•)
```

**ç°ä»£Actorç³»ç»Ÿçš„å…³é”®ç‰¹æ€§åˆ†æ**:

1. **é›†ç¾¤æ„ŸçŸ¥ (Cluster Awareness)**:
   ```scala
   // é›†ç¾¤æˆå‘˜å…³ç³»çš„æ•°å­¦æ¨¡å‹
   case class ClusterView(
     members: Set[Member],
     leader: Option[Address],
     seenBy: Set[Address]
   )
   
   // æˆå‘˜çŠ¶æ€è½¬æ¢
   sealed trait MemberStatus
   case object Joining extends MemberStatus
   case object Up extends MemberStatus
   case object Leaving extends MemberStatus
   case object Exiting extends MemberStatus
   case object Down extends MemberStatus
   ```

2. **ä½ç½®é€æ˜æ€§ (Location Transparency)**:
   - **æœ¬åœ°ä¼˜åŒ–**: è‡ªåŠ¨æ£€æµ‹æœ¬åœ°Actorï¼Œé¿å…ç½‘ç»œå¼€é”€
   - **åºåˆ—åŒ–ç­–ç•¥**: æ™ºèƒ½é€‰æ‹©åºåˆ—åŒ–æ–¹å¼
   - **è·¯ç”±æœºåˆ¶**: åŸºäºä¸€è‡´æ€§å“ˆå¸Œçš„Actorè·¯ç”±

3. **å®¹é”™æœºåˆ¶ (Fault Tolerance)**:
   ```scala
   // ç›‘ç£ç­–ç•¥çš„å½¢å¼åŒ–å®šä¹‰
   sealed trait SupervisorStrategy {
     def handle(child: ActorRef, exception: Throwable): Directive
   }
   
   sealed trait Directive
   case object Resume extends Directive      // æ¢å¤Actor
   case object Restart extends Directive     // é‡å¯Actor
   case object Stop extends Directive        // åœæ­¢Actor
   case object Escalate extends Directive   // ä¸ŠæŠ¥é”™è¯¯
   ```

4. **æŒä¹…åŒ– (Persistence)**:
   - **Event Sourcing**: çŠ¶æ€å˜æ›´çš„äº‹ä»¶æ—¥å¿—
   - **Snapshotting**: å®šæœŸçŠ¶æ€å¿«ç…§
   - **Recovery**: åŸºäºäº‹ä»¶æ—¥å¿—çš„çŠ¶æ€é‡å»º

#### ğŸ”¬ æ·±å…¥ç ”ç©¶é—®é¢˜ä¸å¼€æ”¾æ€§è¯¾é¢˜

**1. æ¶ˆæ¯ä¼ é€’è¯­ä¹‰çš„ç†è®ºè¾¹ç•Œ**:
```scala
// æ¶ˆæ¯ä¼ é€’çš„ä¸‰ä¸ªè¯­ä¹‰å±‚æ¬¡åŠå…¶ç†è®ºä¿è¯
sealed trait DeliveryGuarantee {
  def theoreticalGuarantee: String
  def implementationComplexity: Int
  def performanceOverhead: Double
}

case object AtMostOnce extends DeliveryGuarantee {
  val theoreticalGuarantee = "æ¶ˆæ¯å¯èƒ½ä¸¢å¤±ï¼Œä½†ä¸ä¼šé‡å¤"
  val implementationComplexity = 1  // æœ€ç®€å•
  val performanceOverhead = 0.0     // æ— å¼€é”€
}

case object AtLeastOnce extends DeliveryGuarantee {
  val theoreticalGuarantee = "æ¶ˆæ¯å¯èƒ½é‡å¤ï¼Œä½†ä¸ä¼šä¸¢å¤±"
  val implementationComplexity = 2  // ä¸­ç­‰
  val performanceOverhead = 0.1     // è½»å¾®å¼€é”€
}

case object ExactlyOnce extends DeliveryGuarantee {
  val theoreticalGuarantee = "æ¶ˆæ¯æ—¢ä¸ä¸¢å¤±ä¹Ÿä¸é‡å¤"
  val implementationComplexity = 3  // æœ€å¤æ‚
  val performanceOverhead = 0.3     // æ˜¾è‘—å¼€é”€
}
```

**ç†è®ºç ”ç©¶é—®é¢˜**:
- åœ¨åˆ†å¸ƒå¼Actorç³»ç»Ÿä¸­ï¼Œç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰çš„ç†è®ºä¸‹ç•Œæ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•åœ¨ä¿è¯è¯­ä¹‰çš„å‰æä¸‹ï¼Œæœ€å°åŒ–å®ç°å¤æ‚åº¦ï¼Ÿ
- å¼‚æ­¥ç½‘ç»œç¯å¢ƒä¸‹çš„æ¶ˆæ¯é¡ºåºæ€§ç†è®ºä¿è¯

**2. Actorç›‘ç£ç­–ç•¥çš„æ•°å­¦å»ºæ¨¡**:
```scala
// ç›‘ç£ç­–ç•¥çš„æ•°å­¦æ¨¡å‹ï¼ˆåŸºäºè‡ªåŠ¨æœºç†è®ºï¼‰
abstract class SupervisorAutomaton {
  // çŠ¶æ€ç©ºé—´
  type State = (Set[ActorRef], SupervisorConfig)
  
  // è½¬ç§»å‡½æ•°
  def transition(state: State, event: SupervisorEvent): State
  
  // æ¥å—è¯­è¨€
  def accepts(trace: List[SupervisorEvent]): Boolean
}

// ç›‘ç£äº‹ä»¶çš„åˆ†ç±»
sealed trait SupervisorEvent
case class ChildFailure(actor: ActorRef, exception: Throwable) extends SupervisorEvent
case class ChildRestarted(actor: ActorRef) extends SupervisorEvent
case class ChildTerminated(actor: ActorRef) extends SupervisorEvent
```

**å¼€æ”¾æ€§ç ”ç©¶è¯¾é¢˜**:
- **è‡ªé€‚åº”ç›‘ç£ç­–ç•¥**: åŸºäºæœºå™¨å­¦ä¹ çš„åŠ¨æ€ç›‘ç£ç­–ç•¥é€‰æ‹©
- **å±‚æ¬¡åŒ–ç›‘ç£**: å¤§è§„æ¨¡Actorç³»ç»Ÿçš„ç›‘ç£å±‚æ¬¡ä¼˜åŒ–
- **è·¨èŠ‚ç‚¹ç›‘ç£**: åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„ç›‘ç£åè°ƒæœºåˆ¶

**3. Actorç³»ç»Ÿçš„å¯æ‰©å±•æ€§ç†è®º**:
```scala
// å¯æ‰©å±•æ€§çš„æ•°å­¦åº¦é‡
case class ScalabilityMetrics(
  throughputFunction: NodeCount â‡’ Throughput,    // ååé‡å‡½æ•°
  latencyFunction: NodeCount â‡’ Latency,          // å»¶è¿Ÿå‡½æ•°
  communicationComplexity: MessageCount â‡’ Cost,  // é€šä¿¡å¤æ‚åº¦
  stateDistributionEfficiency: Double            // çŠ¶æ€åˆ†å¸ƒæ•ˆç‡
)

// ç†æƒ³å¯æ‰©å±•æ€§çš„æ•°å­¦å®šä¹‰
def idealScalability(metrics: ScalabilityMetrics): Boolean = {
  // çº¿æ€§æ‰©å±•æ€§ï¼šååé‡éšèŠ‚ç‚¹æ•°çº¿æ€§å¢é•¿
  val linearThroughput = metrics.throughputFunction.isLinear
  
  // å»¶è¿Ÿç¨³å®šæ€§ï¼šå»¶è¿Ÿä¸éšèŠ‚ç‚¹æ•°å¢é•¿
  val stableLatency = metrics.latencyFunction.isConstant
  
  linearThroughput && stableLatency
}
```

#### ğŸ§ª å®éªŒéªŒè¯æ–¹æ³•

**1. æ¶ˆæ¯ä¼ é€’è¯­ä¹‰éªŒè¯å®éªŒ**:
```scala
// å®éªŒè®¾è®¡ï¼šç½‘ç»œåˆ†åŒºä¸‹çš„æ¶ˆæ¯ä¼ é€’æµ‹è¯•
class MessageDeliveryExperiment {
  def testAtMostOnce(): ExperimentResult = {
    // æ¨¡æ‹Ÿç½‘ç»œåˆ†åŒºï¼ŒéªŒè¯æ¶ˆæ¯ä¸¢å¤±ä½†ä¸é‡å¤
  }
  
  def testAtLeastOnce(): ExperimentResult = {
    // æ¨¡æ‹Ÿæ¶ˆæ¯é‡ä¼ ï¼ŒéªŒè¯æ¶ˆæ¯é‡å¤ä½†ä¸ä¸¢å¤±
  }
  
  def testExactlyOnce(): ExperimentResult = {
    // ä½¿ç”¨äº‹åŠ¡æ€§æ¶ˆæ¯ï¼ŒéªŒè¯ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰
  }
}
```

**2. ç›‘ç£ç­–ç•¥æ€§èƒ½åŸºå‡†**:
```scala
// ç›‘ç£ç­–ç•¥çš„æ€§èƒ½å¯¹æ¯”å®éªŒ
class SupervisorStrategyBenchmark {
  def strategies: List[SupervisorStrategy] = List(
    OneForOneStrategy(),
    OneForAllStrategy(), 
    AllForOneStrategy()
  )
  
  def benchmark(): BenchmarkResult = {
    // æµ‹è¯•æŒ‡æ ‡ï¼šæ¢å¤æ—¶é—´ã€èµ„æºå¼€é”€ã€é”™è¯¯ä¼ æ’­èŒƒå›´
  }
}
```

---

### ğŸ¯ 1.2 ä¸€è‡´æ€§ç†è®ºæ·±åº¦è§£æ

#### ğŸ“Š CAPå®šç†çš„æ•°å­¦åŸºç¡€ä¸æ‰©å±•

**"Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services" (Gilbert & Lynch, 2002)**
- **ä½œè€…**: Seth Gilbert, Nancy Lynch
- **å‘è¡¨**: SIGACT 2002
- **è´¡çŒ®**: å°†Brewerçš„çŒœæƒ³è¿›è¡Œä¸¥æ ¼æ•°å­¦è¯æ˜
- **å¼•ç”¨**: ~8000+ å¼•ç”¨ï¼Œåˆ†å¸ƒå¼ç³»ç»Ÿç†è®ºçš„é‡Œç¨‹ç¢‘

**CAPå®šç†å½¢å¼åŒ–å®šä¹‰**:
```
è®¾åˆ†å¸ƒå¼ç³»ç»ŸS = {Nâ‚, Nâ‚‚, ..., Nâ‚™}ï¼Œåœ¨å­˜åœ¨ç½‘ç»œåˆ†åŒºçš„æƒ…å†µä¸‹ï¼š

ä¸€è‡´æ€§(Consistency, C): 
âˆ€i,j âˆˆ [1,n], âˆ€t: view(Náµ¢, t) = view(Nâ±¼, t)
å³æ‰€æœ‰èŠ‚ç‚¹åœ¨ä»»ä½•æ—¶åˆ»çœ‹åˆ°ç›¸åŒçš„æ•°æ®è§†å›¾

å¯ç”¨æ€§(Availability, A): 
âˆ€i âˆˆ [1,n], âˆ€req: âˆƒresp âˆˆ [t, t+Î”]
å³æ¯ä¸ªè¯·æ±‚éƒ½èƒ½åœ¨æœ‰é™æ—¶é—´å†…æ”¶åˆ°å“åº”

åˆ†åŒºå®¹é”™æ€§(Partition Tolerance, P): 
âˆƒpartition: network = componentâ‚ âˆª componentâ‚‚, componentâ‚ âˆ© componentâ‚‚ = âˆ…
ç³»ç»Ÿåœ¨ç½‘ç»œåˆ†åŒºæ—¶ä»èƒ½ç»§ç»­è¿è¡Œ

å®šç†: åœ¨å¼‚æ­¥ç½‘ç»œä¸­ï¼ŒCAPä¸èƒ½åŒæ—¶æ»¡è¶³
```

**æ•°å­¦è¯æ˜çš„è¯¦ç»†æ­¥éª¤**:

1. **ç³»ç»Ÿæ¨¡å‹å®šä¹‰**:
```scala
// åˆ†å¸ƒå¼ç³»ç»Ÿçš„å½¢å¼åŒ–æ¨¡å‹
case class DistributedSystem(
  nodes: Set[Node],
  network: NetworkModel,
  failureModel: FailureModel
)

// ç½‘ç»œæ¨¡å‹ï¼šå¼‚æ­¥ç½‘ç»œ + æ¶ˆæ¯å»¶è¿Ÿæœ‰ç•Œä½†æœªçŸ¥
case class AsynchronousNetwork(
  maxDelay: Option[Time] = None,  // å»¶è¿Ÿæœ‰ç•Œä½†æœªçŸ¥
  messageLoss: Boolean = false,    // æ— æ¶ˆæ¯ä¸¢å¤±
  networkPartition: Boolean = true // å¯èƒ½å‘ç”Ÿåˆ†åŒº
)
```

2. **ä¸å¯èƒ½æ€§è¯æ˜æ„é€ **:
```scala
// æ„é€ åä¾‹ï¼šä¸¤èŠ‚ç‚¹ç³»ç»Ÿçš„ä¸€è‡´æ€§-å¯ç”¨æ€§å†²çª
class CAPImpossibilityProof {
  // èŠ‚ç‚¹Nâ‚å’ŒNâ‚‚ï¼Œåˆå§‹å€¼vâ‚€
  // å®¢æˆ·ç«¯Câ‚å‘Nâ‚å†™å…¥vâ‚ï¼Œå®¢æˆ·ç«¯Câ‚‚å‘Nâ‚‚å†™å…¥vâ‚‚
  // ç½‘ç»œåˆ†åŒºï¼šNâ‚å’ŒNâ‚‚æ— æ³•é€šä¿¡
  
  def proveCAPConflict(): Unit = {
    // å¦‚æœä¿è¯ä¸€è‡´æ€§ï¼š
    // Nâ‚å’ŒNâ‚‚å¿…é¡»è¾¾æˆå…±è¯†ï¼Œä½†ç½‘ç»œåˆ†åŒºé˜»æ­¢äº†é€šä¿¡
    // å› æ­¤è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹å¿…é¡»æ‹’ç»è¯·æ±‚ â†’ è¿åå¯ç”¨æ€§
    
    // å¦‚æœä¿è¯å¯ç”¨æ€§ï¼š
    // Nâ‚å’ŒNâ‚‚éƒ½å¿…é¡»å“åº”è¯·æ±‚ï¼Œä½†æ— æ³•ä¿è¯å€¼çš„ä¸€è‡´æ€§
    // å› æ­¤å¯èƒ½äº§ç”Ÿä¸ä¸€è‡´çš„çŠ¶æ€ â†’ è¿åä¸€è‡´æ€§
  }
}
```

#### ğŸ§® FLPä¸å¯èƒ½æ€§å®šç†æ·±åº¦è§£æ

**"Impossibility of Distributed Consensus with One Faulty Process" (Fischer, Lynch, & Paterson, 1985)**
- **å‘è¡¨**: PODC 1985ï¼Œè·å¾—Dijkstraå¥–
- **æ ¸å¿ƒç»“è®º**: åœ¨å¼‚æ­¥åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹å¯èƒ½å¤±è´¥ï¼Œä¹Ÿä¸å­˜åœ¨ç¡®å®šæ€§ç®—æ³•èƒ½å¤Ÿè§£å†³å…±è¯†é—®é¢˜

**FLPå®šç†çš„ç²¾ç¡®è¡¨è¿°**:
```
ç»™å®šå¼‚æ­¥åˆ†å¸ƒå¼ç³»ç»ŸSï¼Œæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
1. ç½‘ç»œæ˜¯å¼‚æ­¥çš„ï¼ˆæ¶ˆæ¯ä¼ é€’å»¶è¿Ÿæ— ç•Œï¼‰
2. æœ€å¤šæœ‰ä¸€ä¸ªè¿›ç¨‹å¯èƒ½å´©æºƒï¼ˆcrash failureï¼‰
3. ç³»ç»Ÿæ˜¯éåŒæ­¥çš„ï¼ˆæ²¡æœ‰å…¨å±€æ—¶é’Ÿï¼‰
4. ç®—æ³•æ˜¯ç¡®å®šæ€§çš„

ç»“è®ºï¼šä¸å­˜åœ¨èƒ½å¤Ÿè§£å†³å…±è¯†é—®é¢˜çš„ç®—æ³•
```

**è¯æ˜çš„æ ¸å¿ƒæ€æƒ³**:
```scala
// FLPè¯æ˜çš„å…³é”®æ¦‚å¿µ
class FLPProof {
  // 1. ç³»ç»Ÿé…ç½®ï¼šåŒ…å«æ‰€æœ‰è¿›ç¨‹çš„çŠ¶æ€å’Œæ¶ˆæ¯ç¼“å†²åŒº
  case class Configuration(
    processStates: Map[ProcessId, ProcessState],
    messageBuffer: Multiset[Message]
  )
  
  // 2. é…ç½®çš„å¯è¾¾å…³ç³»ï¼šé€šè¿‡ä¸€æ­¥è®¡ç®—å¯ä»¥åˆ°è¾¾çš„é…ç½®
  def reachable(config: Configuration): Set[Configuration]
  
  // 3. å†³å®šæ€§é…ç½®ï¼šå·²ç»å†³å®šäº†å€¼çš„é…ç½®
  def isDecided(config: Configuration, value: Value): Boolean
  
  // 4. æœªå†³å®šæ€§é…ç½®ï¼šè¿˜æœªå†³å®šå€¼çš„é…ç½®
  def isUndecided(config: Configuration): Boolean
  
  // è¯æ˜æ ¸å¿ƒï¼šä»åˆå§‹æœªå†³å®šé…ç½®å¼€å§‹ï¼Œ
  // æ€»å­˜åœ¨ä¸€ä¸ªæ‰§è¡Œè·¯å¾„ä½¿ç³»ç»Ÿä¿æŒæœªå†³å®šçŠ¶æ€
  def proveImpossibility(): Unit = {
    // æ„é€ ä¸€ä¸ªæ— é™æ‰§è¡Œçš„è·¯å¾„ï¼Œç³»ç»Ÿæ°¸è¿œæ— æ³•è¾¾æˆå…±è¯†
  }
}
```

#### ğŸ›ï¸ å…±è¯†ç®—æ³•çš„ç†è®ºæ¼”è¿›

**Paxosç®—æ³•çš„æ•°å­¦åŸºç¡€**:

**"The Part-Time Parliament" (Lamport, 1998)**
- **å‘è¡¨**: ACM Transactions on Computer Systems
- **è´¡çŒ®**: æå‡ºäº†ç¬¬ä¸€ä¸ªå®ç”¨çš„åˆ†å¸ƒå¼å…±è¯†ç®—æ³•
- **ç†è®ºä¿è¯**: å®‰å…¨æ€§(Safety)å’Œæ´»æ€§(Liveness)

**Paxosç®—æ³•çš„å½¢å¼åŒ–å®šä¹‰**:
```scala
// Paxosçš„ä¸‰ä¸ªè§’è‰²
case class PaxosRoles(
  proposers: Set[Proposer],
  acceptors: Set[Acceptor],
  learners: Set[Learner]
)

// æè®®çš„æ•°å­¦ç»“æ„
case class Proposal(
  number: BallotNumber,  // æè®®ç¼–å·
  value: Value          // æè®®å€¼
)

// æ³•å®šäººæ•°(Quorum)çš„å®šä¹‰
def isQuorum(set: Set[Acceptor]): Boolean = {
  set.size > acceptors.size / 2  // å¤šæ•°æ´¾
}

// Paxosçš„ä¸¤ä¸ªé˜¶æ®µ
sealed trait PaxosPhase
case class Prepare(number: BallotNumber) extends PaxosPhase
case class Accept(number: BallotNumber, value: Value) extends PaxosPhase
```

**Paxosçš„å®‰å…¨æ€§è¯æ˜**:
```scala
// Paxoså®‰å…¨æ€§å®šç†
class PaxosSafetyProof {
  // å®šç†ï¼šå¦‚æœå€¼ä¸ºvè¢«chosenï¼Œé‚£ä¹ˆæ‰€æœ‰higher-numberedçš„ææ¡ˆå¿…é¡»æ˜¯v
  def safetyTheorem(): Unit = {
    // è¯æ˜æ€è·¯ï¼š
    // 1. å‡è®¾å­˜åœ¨ä¸¤ä¸ªchosenå€¼vå’Œv'ï¼Œä¸”v â‰  v'
    // 2. æ ¹æ®æ³•å®šäººæ•°æ€§è´¨ï¼Œå¿…ç„¶å­˜åœ¨ä¸€ä¸ªacceptoråŒæ—¶æ¥å—äº†ä¸¤ä¸ªææ¡ˆ
    // 3. è¿™ä¸Paxosåè®®çš„Promiseé˜¶æ®µçŸ›ç›¾
    // 4. å› æ­¤å‡è®¾ä¸æˆç«‹ï¼Œå®‰å…¨æ€§å¾—åˆ°ä¿è¯
  }
}
```

**Raftç®—æ³•çš„ç†è®ºæ”¹è¿›**:

**"In Search of an Understandable Consensus Algorithm" (Ongaro & Ousterhout, 2014)**
- **è´¡çŒ®**: æå‡ºæ›´æ˜“ç†è§£å’Œå®ç°çš„å…±è¯†ç®—æ³•
- **ç†è®ºåˆ›æ–°**: é¢†å¯¼è€…é€‰ä¸¾ + æ—¥å¿—å¤åˆ¶ + å®‰å…¨æ€§

**Raftçš„çŠ¶æ€æœºå½¢å¼åŒ–**:
```scala
// RaftèŠ‚ç‚¹çš„çŠ¶æ€
case class RaftState(
  currentTerm: Long,           // å½“å‰ä»»æœŸ
  votedFor: Option[NodeId],    // æŠ•ç¥¨ç»™çš„å€™é€‰äºº
  log: List[LogEntry],         // æ—¥å¿—æ¡ç›®
  commitIndex: Long,           // å·²æäº¤çš„æ—¥å¿—ç´¢å¼•
  lastApplied: Long,           // æœ€ååº”ç”¨çš„æ—¥å¿—ç´¢å¼•
  role: NodeRole               // èŠ‚ç‚¹è§’è‰²
)

sealed trait NodeRole
case class Follower(leaderId: Option[NodeId]) extends NodeRole
case object Candidate extends NodeRole
case class Leader(matchIndex: Map[NodeId, Long]) extends NodeRole

// Raftçš„å®‰å…¨æ€§ä¸å˜é‡
object RaftInvariants {
  // 1. é€‰ä¸¾å®‰å…¨æ€§ï¼šæ¯ä¸ªä»»æœŸæœ€å¤šä¸€ä¸ªé¢†å¯¼è€…
  def electionSafety(state: RaftState): Boolean
  
  // 2. é¢†å¯¼è€…åªè¿½åŠ ï¼šé¢†å¯¼è€…ä»ä¸åˆ é™¤æˆ–è¦†ç›–è‡ªå·±çš„æ—¥å¿—æ¡ç›®
  def leaderAppendOnly(state: RaftState): Boolean
  
  // 3. æ—¥å¿—åŒ¹é…ï¼šå¦‚æœä¸¤ä¸ªæ—¥å¿—æ¡ç›®æœ‰ç›¸åŒçš„ç´¢å¼•å’Œä»»æœŸï¼Œé‚£ä¹ˆæ—¥å¿—ç›¸åŒ
  def logMatching(state1: RaftState, state2: RaftState): Boolean
  
  // 4. é¢†å¯¼è€…å®Œå¤‡æ€§ï¼šå¦‚æœä¸€ä¸ªæ—¥å¿—æ¡ç›®åœ¨æŸä¸ªä»»æœŸè¢«æäº¤ï¼Œ
  //           é‚£ä¹ˆå®ƒå°†å‡ºç°åœ¨æ‰€æœ‰æ›´é«˜ä»»æœŸçš„é¢†å¯¼è€…æ—¥å¿—ä¸­
  def leaderCompleteness(state: RaftState): Boolean
}
```

#### ğŸ”¬ ç°ä»£ä¸€è‡´æ€§ç†è®ºçš„å‘å±•

**æ‹œå åº­å®¹é”™ç†è®º**:
```scala
// æ‹œå åº­å°†å†›é—®é¢˜çš„å½¢å¼åŒ–å®šä¹‰
class ByzantineGeneralsProblem {
  // nä¸ªå°†å†›ï¼Œæœ€å¤šfä¸ªå›å¾’ï¼Œéœ€è¦è¾¾æˆä¸€è‡´
  // è§£å†³æ–¹æ¡ˆå­˜åœ¨çš„å……è¦æ¡ä»¶ï¼šn > 3f
  
  def solvableCondition(n: Int, f: Int): Boolean = {
    n > 3 * f  // æ‹œå åº­å®¹é”™çš„ç†è®ºä¸‹ç•Œ
  }
  
  // æ¶ˆæ¯å¤æ‚åº¦åˆ†æ
  def messageComplexity(n: Int, f: Int): Int = {
    // ç»å…¸BFTç®—æ³•éœ€è¦O(nÂ²f)æ¡æ¶ˆæ¯
    n * n * f
  }
}

// å®ç”¨æ‹œå åº­å®¹é”™ç®—æ³•(PBFT)
class PracticalBFT {
  // ä¸‰é˜¶æ®µåè®®ï¼špre-prepare, prepare, commit
  sealed trait PBFTPhase
  case class PrePrepare(view: Long, sequenceNumber: Long, digest: Array[Byte]) extends PBFTPhase
  case class Prepare(view: Long, sequenceNumber: Long, digest: Array[Byte]) extends PBFTPhase
  case class Commit(view: Long, sequenceNumber: Long, digest: Array[Byte]) extends PBFTPhase
  
  // æ€§èƒ½åˆ†æï¼šæœ€å¤šå®¹å¿fä¸ªæ•…éšœèŠ‚ç‚¹ï¼Œéœ€è¦3f+1ä¸ªèŠ‚ç‚¹
  def requiredNodes(f: Int): Int = 3 * f + 1
}
```

**æœ€ç»ˆä¸€è‡´æ€§æ¨¡å‹**:
```scala
// æœ€ç»ˆä¸€è‡´æ€§çš„å½¢å¼åŒ–å®šä¹‰
trait EventuallyConsistent {
  // å¦‚æœæ²¡æœ‰æ–°çš„æ›´æ–°ï¼Œæ‰€æœ‰å‰¯æœ¬æœ€ç»ˆä¼šæ”¶æ•›åˆ°ç›¸åŒå€¼
  def eventualConsistency(): Boolean = {
    // æ•°å­¦è¡¨è¾¾ï¼š
    // âˆ€i,j âˆˆ replicas, âˆƒtâ‚€: âˆ€t â‰¥ tâ‚€: value(replica_i, t) = value(replica_j, t)
  }
}

// å†²çªè§£å†³ç­–ç•¥
sealed trait ConflictResolutionStrategy
case object LastWriteWins extends ConflictResolutionStrategy
case object VectorClock extends ConflictResolutionStrategy
case object MerkleTree extends ConflictResolutionStrategy

// å‘é‡æ—¶é’Ÿçš„å®ç°
case class VectorClock(
  clock: Map[NodeId, Long]
) {
  // äº‹ä»¶ååºå…³ç³»
  def happensBefore(other: VectorClock): Boolean = {
    clock.forall { case (node, time) =>
      time <= other.clock.getOrElse(node, 0L)
    } && clock.exists { case (node, time) =>
      time < other.clock.getOrElse(node, 0L)
    }
  }
  
  // å› æœå…³ç³»
  def isConcurrent(other: VectorClock): Boolean = {
    !happensBefore(other) && !other.happensBefore(this)
  }
}
```

#### ğŸ§ª ä¸€è‡´æ€§ç†è®ºçš„å®éªŒéªŒè¯

**å…±è¯†ç®—æ³•æ€§èƒ½å¯¹æ¯”å®éªŒ**:
```scala
class ConsensusBenchmark {
  // æµ‹è¯•ç¯å¢ƒï¼š3-9ä¸ªèŠ‚ç‚¹ï¼Œç½‘ç»œå»¶è¿Ÿ1-100ms
  def benchmarkPaxos(): BenchmarkResult = {
    // æµ‹è¯•æŒ‡æ ‡ï¼š
    // - ååé‡ï¼šæ¯ç§’å¤„ç†çš„å…±è¯†ææ¡ˆæ•°
    // - å»¶è¿Ÿï¼šææ¡ˆè¾¾æˆå…±è¯†çš„æ—¶é—´
    // - æ•…éšœæ¢å¤æ—¶é—´ï¼šé¢†å¯¼è€…æ•…éšœåçš„æ¢å¤æ—¶é—´
  }
  
  def benchmarkRaft(): BenchmarkResult = {
    // ä¸Paxoså¯¹æ¯”ï¼Œæµ‹è¯•æ˜“å®ç°æ€§å¸¦æ¥çš„æ€§èƒ½å·®å¼‚
  }
  
  def benchmarkPBFT(): BenchmarkResult = {
    // æµ‹è¯•æ‹œå åº­å®¹é”™çš„æ€§èƒ½å¼€é”€
  }
}

// ä¸€è‡´æ€§çº§åˆ«çš„é‡åŒ–æµ‹é‡
class ConsistencyLevelMeasurement {
  def measureConsistency(window: TimeWindow): ConsistencyMetrics = {
    ConsistencyMetrics(
      staleness = maxDataAge(window),          // æ•°æ®é™ˆæ—§åº¦
      divergence = replicaDivergence(window),  // å‰¯æœ¬åˆ†æ­§åº¦
      convergenceTime = timeToConverge()       // æ”¶æ•›æ—¶é—´
    )
  }
}
```

---

### ğŸ›¡ï¸ 1.3 å®¹é”™æœºåˆ¶ç†è®ºæ·±åº¦è§£æ

#### ğŸ­ æ‹œå åº­å®¹é”™ç†è®ºçš„æ•°å­¦åŸºç¡€

**æ‹œå åº­å°†å†›é—®é¢˜çš„å½¢å¼åŒ–å®šä¹‰**:
```
é—®é¢˜è®¾å®šï¼š
- nä¸ªå°†å†›å›´æ”»ä¸€ä¸ªåŸå¸‚ï¼Œéœ€è¦è¾¾æˆè¿›æ”»/æ’¤é€€çš„ä¸€è‡´å†³å®š
- å…¶ä¸­æœ€å¤šfä¸ªå°†å†›æ˜¯å›å¾’ï¼Œå¯èƒ½å‘é€çŸ›ç›¾çš„æ¶ˆæ¯
- å›å¾’ä¹‹é—´çš„åè°ƒæ˜¯å®Œç¾çš„ï¼Œå¿ è¯šå°†å†›ä¹‹é—´é€šä¿¡ä¸å¯é 

æ•°å­¦è¡¨è¾¾ï¼š
ç»™å®šç³»ç»ŸS = {Gâ‚, Gâ‚‚, ..., Gâ‚™}ï¼Œ|S| = n
æ•…éšœé›†åˆF âŠ† S, |F| â‰¤ f
ç›®æ ‡ï¼šâˆ€gáµ¢, gâ±¼ âˆˆ S\F: decision(gáµ¢) = decision(gâ±¼)

å¯è§£æ€§æ¡ä»¶ï¼šn > 3f
```

**ç†è®ºè¯æ˜**:
```scala
// æ‹œå åº­å®¹é”™çš„ä¸‹ç•Œè¯æ˜
class ByzantineLowerBoundProof {
  // è¯æ˜ï¼šå¦‚æœn â‰¤ 3fï¼Œé—®é¢˜æ— è§£
  def proveImpossibility(n: Int, f: Int): Boolean = {
    if (n <= 3 * f) {
      // æ„é€ åä¾‹ï¼šå°†å°†å†›åˆ†æˆä¸‰ç»„
      // Aç»„ï¼šå¿ è¯šå°†å†›ï¼Œçœ‹åˆ°æ¥è‡ªBå’ŒCçš„çŸ›ç›¾æ¶ˆæ¯
      // Bç»„ï¼šå›å¾’ï¼Œå‘Aå‘é€è¿›æ”»ï¼Œå‘Cå‘é€æ’¤é€€
      // Cç»„ï¼šå¿ è¯šå°†å†›ï¼Œçœ‹åˆ°æ¥è‡ªAå’ŒBçš„çŸ›ç›¾æ¶ˆæ¯
      // Aå’ŒCæ— æ³•è¾¾æˆä¸€è‡´
      false
    } else {
      true  // æœ‰è§£
    }
  }
  
  // æ¶ˆæ¯å¤æ‚åº¦åˆ†æ
  def messageComplexity(n: Int, f: Int): BigO = {
    // é€’å½’ç®—æ³•ï¼šm(n,f) = m(n-1,f) + m(n-f-1,f-1) + O(1)
    // è§£å¾—ï¼šm(n,f) = O(n^f * f!)
    BigO(s"n^${f} * f!")
  }
}
```

#### ğŸ” æ•…éšœæ£€æµ‹ç†è®ºçš„æ•°å­¦å»ºæ¨¡

**æ•…éšœæ£€æµ‹å™¨çš„åˆ†ç±»ä½“ç³»**:
```scala
// æ•…éšœæ£€æµ‹å™¨çš„å½¢å¼åŒ–å®šä¹‰
abstract class FailureDetector {
  // æ£€æµ‹å™¨è¾“å‡ºï¼šæ€€ç–‘é›†åˆ
  def suspected(): Set[ProcessId]
  
  // æ£€æµ‹å™¨å±æ€§
  def completeness(): CompletenessProperty
  def accuracy(): AccuracyProperty
}

// å®Œæ•´æ€§å±æ€§ï¼šæœ€ç»ˆæ‰€æœ‰æ•…éšœè¿›ç¨‹éƒ½è¢«æ€€ç–‘
sealed trait CompletenessProperty
case object StrongCompleteness extends CompletenessProperty {
  // âˆ€p âˆˆ crashed: âˆƒt: âˆ€t' â‰¥ t: p âˆˆ suspected(t')
}
case object WeakCompleteness extends CompletenessProperty {
  // âˆƒp âˆˆ crashed: âˆƒt: âˆ€t' â‰¥ t: p âˆˆ suspected(t')
}
case object EventuallyStrongCompleteness extends CompletenessProperty {
  // âˆ€p âˆˆ crashed: âˆƒtâ‚€: âˆ€t â‰¥ tâ‚€: p âˆˆ suspected(t)
}

// å‡†ç¡®æ€§å±æ€§ï¼šæ­£ç¡®è¿›ç¨‹ä¸ä¼šè¢«æ°¸ä¹…æ€€ç–‘
sealed trait AccuracyProperty
case object StrongAccuracy extends AccuracyProperty {
  // âˆ€p âˆˆ correct: p âˆ‰ suspected(t) for all t
}
case object WeakAccuracy extends AccuracyProperty {
  // âˆƒp âˆˆ correct: p âˆ‰ suspected(t) for all t
}
case object EventuallyStrongAccuracy extends AccuracyProperty {
  // âˆƒp âˆˆ correct: âˆƒtâ‚€: âˆ€t â‰¥ tâ‚€: p âˆ‰ suspected(t)
}
```

**Î¦ Accrualæ•…éšœæ£€æµ‹å™¨çš„æ•°å­¦åŸç†**:
```scala
// åŸºäºç»Ÿè®¡çš„æ•…éšœæ£€æµ‹ç®—æ³•
class PhiAccrualDetector {
  // å¿ƒè·³é—´éš”çš„å†å²æ ·æœ¬
  case class HeartbeatHistory(
    samples: Queue[Double],
    mean: Double,
    variance: Double
  )
  
  // Î¦å€¼çš„è®¡ç®—ï¼šåŸºäºæ­£æ€åˆ†å¸ƒçš„åç¦»åº¦
  def phi(lastHeartbeat: Long, currentTime: Long, history: HeartbeatHistory): Double = {
    val delta = currentTime - lastHeartbeat
    val mean = history.mean
    val variance = history.variance
    
    // Î¦ = -logâ‚â‚€(P(heartbeat interval > delta))
    // å‡è®¾å¿ƒè·³é—´éš”æœä»æ­£æ€åˆ†å¸ƒ N(Î¼, ÏƒÂ²)
    val probability = 1.0 - normalCDF(delta, mean, math.sqrt(variance))
    -math.log10(probability)
  }
  
  // æ•…éšœåˆ¤å®šï¼šå¦‚æœÎ¦è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºè¿›ç¨‹æ•…éšœ
  def isSuspected(phi: Double, threshold: Double): Boolean = phi > threshold
}
```

#### ğŸ“‹ çŠ¶æ€å¤åˆ¶ç†è®ºçš„å½¢å¼åŒ–åˆ†æ

**ä¸»åŠ¨å¤åˆ¶ vs è¢«åŠ¨å¤åˆ¶çš„æ•°å­¦å¯¹æ¯”**:
```scala
// å¤åˆ¶ç­–ç•¥çš„å½¢å¼åŒ–å®šä¹‰
sealed trait ReplicationStrategy {
  def faultTolerance(n: Int): Int  // å®¹é”™èƒ½åŠ›
  def messageComplexity(n: Int): BigO  // æ¶ˆæ¯å¤æ‚åº¦
  def consistencyGuarantee: ConsistencyLevel  // ä¸€è‡´æ€§ä¿è¯
}

case object ActiveReplication extends ReplicationStrategy {
  // æ‰€æœ‰å‰¯æœ¬å¤„ç†ç›¸åŒè¯·æ±‚ï¼Œéœ€è¦ç¡®å®šæ€§çš„è¯·æ±‚å¤„ç†
  def faultTolerance(n: Int): Int = (n - 1) / 2  // f < (n-1)/2
  
  def messageComplexity(n: Int): BigO = {
    // å®¢æˆ·ç«¯ -> ä¸»å‰¯æœ¬: 1æ¡æ¶ˆæ¯
    // ä¸»å‰¯æœ¬ -> æ‰€æœ‰å‰¯æœ¬: n-1æ¡æ¶ˆæ¯
    // å‰¯æœ¬ -> å®¢æˆ·ç«¯: n-1æ¡æ¶ˆæ¯
    BigO("2n-1")
  }
  
  def consistencyGuarantee: ConsistencyLevel = StrongConsistency
}

case object PassiveReplication extends ReplicationStrategy {
  // ä¸»å‰¯æœ¬å¤„ç†è¯·æ±‚ï¼ŒåŒæ­¥çŠ¶æ€åˆ°ä»å‰¯æœ¬
  def faultTolerance(n: Int): Int = n - 1  // f < n-1
  
  def messageComplexity(n: Int): BigO = {
    // å®¢æˆ·ç«¯ -> ä¸»å‰¯æœ¬: 1æ¡æ¶ˆæ¯
    // ä¸»å‰¯æœ¬ -> ä»å‰¯æœ¬: n-1æ¡æ¶ˆæ¯ï¼ˆçŠ¶æ€åŒæ­¥ï¼‰
    BigO("n")
  }
  
  def consistencyGuarantee: ConsistencyLevel = StrongConsistency
}

// ä¸€è‡´æ€§çº§åˆ«çš„å½¢å¼åŒ–å®šä¹‰
sealed trait ConsistencyLevel
case object StrongConsistency extends ConsistencyLevel
case object EventualConsistency extends ConsistencyLevel
case object WeakConsistency extends ConsistencyLevel
```

#### ğŸ§ª å®¹é”™æœºåˆ¶çš„å®éªŒéªŒè¯æ¡†æ¶

**æ•…éšœæ³¨å…¥å®éªŒè®¾è®¡**:
```scala
// ç³»ç»ŸåŒ–çš„æ•…éšœæ³¨å…¥æµ‹è¯•
class FaultInjectionExperiment {
  // æ•…éšœç±»å‹åˆ†ç±»
  sealed trait FaultType
case class CrashFault(nodeId: NodeId, time: Time) extends FaultType
case class NetworkPartition(nodes1: Set[NodeId], nodes2: Set[NodeId]) extends FaultType
case class MessageDelay(source: NodeId, target: NodeId, delay: Duration) extends FaultType
case class ByzantineFault(nodeId: NodeId, behavior: FaultyBehavior) extends FaultType
  
  // å®éªŒæŒ‡æ ‡
  case class ExperimentMetrics(
    availability: Double,        // ç³»ç»Ÿå¯ç”¨æ€§
    consistencyScore: Double,    // ä¸€è‡´æ€§å¾—åˆ†
    recoveryTime: Duration,      // æ•…éšœæ¢å¤æ—¶é—´
    messageOverhead: Int         // æ¶ˆæ¯å¼€é”€
  )
  
  def runExperiment(
    system: DistributedSystem,
    faults: List[FaultType],
    duration: Duration
  ): ExperimentMetrics = {
    // 1. å»ºç«‹ç³»ç»ŸåŸºçº¿
    // 2. æ³¨å…¥æ•…éšœ
    // 3. ç›‘æ§ç³»ç»Ÿè¡Œä¸º
    // 4. æ”¶é›†æ€§èƒ½æŒ‡æ ‡
    // 5. åˆ†æå®¹é”™æ•ˆæœ
  }
}
```

---

## âš¡ 2. å¤§æ•°æ®æŸ¥è¯¢ä¼˜åŒ–ç†è®º

### ğŸŒ‹ 2.1 Volcano/Cascadesä¼˜åŒ–å™¨æ¡†æ¶çš„æ•°å­¦åŸºç¡€

#### ğŸ›ï¸ Volcanoä¼˜åŒ–å™¨çš„ç†è®ºåŸºç¡€

**"The Volcano Optimizer Generator: Extensibility and Efficient Search" (Graefe, 1994)**
- **å‘è¡¨**: IEEE Data Engineering Bulletin
- **è´¡çŒ®**: å»ºç«‹äº†ç°ä»£æŸ¥è¯¢ä¼˜åŒ–å™¨çš„é€šç”¨æ¡†æ¶
- **ç†è®ºåˆ›æ–°**: åŠ¨æ€è§„åˆ’ + è®°å¿†åŒ–æœç´¢ + è§„åˆ™ç³»ç»Ÿ

**ä¼˜åŒ–å™¨çš„æ•°å­¦æ¨¡å‹**:
```scala
// æŸ¥è¯¢ä¼˜åŒ–å™¨çš„å½¢å¼åŒ–å®šä¹‰
case class QueryOptimizer(
  logicalSpace: LogicalQuerySpace,      // é€»è¾‘æŸ¥è¯¢ç©ºé—´
  physicalSpace: PhysicalQuerySpace,    // ç‰©ç†æŸ¥è¯¢ç©ºé—´
  transformationRules: Set[Rule],       // è½¬æ¢è§„åˆ™é›†
  costModel: CostFunction,              // æˆæœ¬å‡½æ•°
  searchStrategy: SearchStrategy        // æœç´¢ç­–ç•¥
)

// æŸ¥è¯¢ç©ºé—´çš„æ•°å­¦ç»“æ„
abstract class QuerySpace[T] {
  def elements: Set[T]                    // ç©ºé—´ä¸­çš„å…ƒç´ 
  def neighbors(element: T): Set[T]       // é‚»å±…å…³ç³»
  def cost(element: T): Cost              // å…ƒç´ æˆæœ¬
  def optimal: Option[T]                  // æœ€ä¼˜å…ƒç´ 
}

// é€»è¾‘æŸ¥è¯¢ç©ºé—´ï¼šç­‰ä»·çš„é€»è¾‘è¡¨è¾¾å¼é›†åˆ
class LogicalQuerySpace extends QuerySpace[LogicalExpression] {
  // é€»è¾‘ç­‰ä»·æ€§ï¼šä¸¤ä¸ªè¡¨è¾¾å¼äº§ç”Ÿç›¸åŒç»“æœ
  def isEquivalent(expr1: LogicalExpression, expr2: LogicalExpression): Boolean
}

// ç‰©ç†æŸ¥è¯¢ç©ºé—´ï¼šç›¸åŒé€»è¾‘çš„ä¸åŒç‰©ç†å®ç°
class PhysicalQuerySpace extends QuerySpace[PhysicalPlan] {
  // ç‰©ç†ç­‰ä»·æ€§ï¼šç›¸åŒçš„æ‰§è¡Œè¯­ä¹‰
  def isEquivalent(plan1: PhysicalPlan, plan2: PhysicalPlan): Boolean
}
```

**åŠ¨æ€è§„åˆ’ç®—æ³•çš„æ•°å­¦åˆ†æ**:
```scala
// Volcanoä¼˜åŒ–å™¨çš„æ ¸å¿ƒç®—æ³•ï¼ˆåŸºäºåŠ¨æ€è§„åˆ’ï¼‰
class VolcanoOptimizer {
  // è®°å¿†åŒ–è¡¨ï¼šé¿å…é‡å¤è®¡ç®—
  val memoTable: mutable.Map[GroupExpression, Group] = mutable.Map()
  
  // ä¼˜åŒ–å‡½æ•°ï¼šé€’å½’åŠ¨æ€è§„åˆ’
  def optimize(group: Group): PhysicalPlan = {
    // 1. æ£€æŸ¥è®°å¿†åŒ–è¡¨
    if (group.bestPlan.isDefined) return group.bestPlan.get
    
    // 2. å±•å¼€æ‰€æœ‰é€»è¾‘è¡¨è¾¾å¼
    val logicalExprs = group.explodeLogical()
    
    // 3. å¯¹æ¯ä¸ªé€»è¾‘è¡¨è¾¾å¼ï¼Œç”Ÿæˆæ‰€æœ‰ç‰©ç†å®ç°
    val allPlans = logicalExprs.flatMap { expr =>
      val physicalImpls = generatePhysicalImplementations(expr)
      physicalImpls.map(impl => optimizePhysicalPlan(impl))
    }
    
    // 4. é€‰æ‹©æˆæœ¬æœ€ä½çš„è®¡åˆ’
    val bestPlan = allPlans.minBy(_.estimatedCost)
    group.bestPlan = Some(bestPlan)
    bestPlan
  }
  
  // æ—¶é—´å¤æ‚åº¦åˆ†æï¼šO(|G| Ã— |R| Ã— |P|)
  // |G|: Groupæ•°é‡, |R|: è§„åˆ™æ•°é‡, |P|: ç‰©ç†å®ç°æ•°é‡
}
```

#### ğŸ¯ Cascadesæ¡†æ¶çš„ç†è®ºæ”¹è¿›

**è®°å¿†åŒ–æœç´¢çš„æ•°å­¦åŸç†**:
```scala
// Cascadesçš„è®°å¿†åŒ–ç»“æ„ï¼ˆæ›´é«˜æ•ˆçš„æœç´¢ç©ºé—´ç®¡ç†ï¼‰
case class MemoStructure(
  groups: Map[GroupExpression, GroupID],    // è¡¨è¾¾å¼åˆ°ç»„çš„æ˜ å°„
  groupMemo: Map[GroupID, Group],            // ç»„çš„è¯¦ç»†ä¿¡æ¯
  expressionMemo: Map[ExprID, GroupExpression] // è¡¨è¾¾å¼IDåˆ°è¡¨è¾¾å¼çš„æ˜ å°„
) {
  // ç¡®ä¿ç­‰ä»·è¡¨è¾¾å¼æ˜ å°„åˆ°åŒä¸€ä¸ªç»„
  def ensureGroup(expr: GroupExpression): GroupID = {
    groups.get(expr) match {
      case Some(groupId) => groupId
      case None =>
        val newGroup = createGroup(expr)
        val groupId = newGroup.id
        groups += (expr -> groupId)
        groupMemo += (groupId -> newGroup)
        groupId
    }
  }
}

// ç»„çš„æ•°å­¦å®šä¹‰ï¼šåŒ…å«æ‰€æœ‰ç­‰ä»·çš„è¡¨è¾¾å¼
case class Group(
  id: GroupID,
  logicalExprs: Set[LogicalExpression],    // é€»è¾‘è¡¨è¾¾å¼é›†åˆ
  physicalExprs: Set[PhysicalExpression],  // ç‰©ç†è¡¨è¾¾å¼é›†åˆ
  bestPlan: Option[PhysicalPlan],          // æœ€ä¼˜ç‰©ç†è®¡åˆ’
  costLowerBound: Cost                     // æˆæœ¬ä¸‹ç•Œ
) {
  // æˆæœ¬ä¸‹ç•Œçš„è®¡ç®—ï¼šç”¨äºå‰ªææœç´¢ç©ºé—´
  def updateCostLowerBound(): Cost = {
    val logicalCosts = logicalExprs.map(minimumLogicalCost)
    val physicalCosts = physicalExprs.map(minimumPhysicalCost)
    costLowerBound = (logicalCosts ++ physicalCosts).min
  }
}
```

**ä¼˜åŒ–è¿‡ç¨‹çš„æ•°å­¦æè¿°**:
```scala
// Cascadesä¼˜åŒ–è¿‡ç¨‹çš„å››ä¸ªé˜¶æ®µ
object CascadesOptimizationPhases {
  // é˜¶æ®µ1ï¼šé€»è¾‘è¡¨è¾¾å¼åˆ†è§£
  def logicalDecomposition(memo: MemoStructure, expr: LogicalExpression): Unit = {
    // åº”ç”¨é€»è¾‘è½¬æ¢è§„åˆ™ï¼Œç”Ÿæˆç­‰ä»·çš„é€»è¾‘è¡¨è¾¾å¼
    val transformations = applyLogicalRules(expr)
    transformations.foreach(transformed => memo.ensureGroup(transformed))
  }
  
  // é˜¶æ®µ2ï¼šç‰©ç†è¡¨è¾¾å¼ç”Ÿæˆ
  def physicalGeneration(memo: MemoStructure, group: Group): Unit = {
    // å¯¹æ¯ä¸ªé€»è¾‘è¡¨è¾¾å¼ï¼Œåº”ç”¨ç‰©ç†å®ç°è§„åˆ™
    group.logicalExprs.foreach { logicalExpr =>
      val physicalImpls = applyPhysicalRules(logicalExpr)
      physicalImpls.foreach(group.addPhysicalExpression)
    }
  }
  
  // é˜¶æ®µ3ï¼šæœ€ä¼˜è®¡åˆ’æœç´¢
  def optimalPlanSearch(memo: MemoStructure, groupId: GroupID): PhysicalPlan = {
    val group = memo.groupMemo(groupId)
    
    // ä½¿ç”¨åˆ†æ”¯å®šç•Œæ³•æœç´¢æœ€ä¼˜è®¡åˆ’
    val searchSpace = enumeratePlans(group)
    val (bestPlan, bestCost) = branchAndBound(searchSpace)
    
    group.bestPlan = Some(bestPlan)
    bestPlan
  }
  
  // é˜¶æ®µ4ï¼šæˆæœ¬ä¼°ç®—ä¼˜åŒ–
  def costEstimationOptimization(memo: MemoStructure): Unit = {
    // è¿­ä»£æ”¹è¿›æˆæœ¬ä¼°ç®—çš„å‡†ç¡®æ€§
    memo.groupMemo.values.foreach { group =>
      refineCostEstimates(group)
    }
  }
}
```

#### ğŸ”¬ ç‰©ç†å±æ€§æ¨å¯¼çš„ç†è®ºåŸºç¡€

**å±æ€§æ¨å¯¼ç³»ç»Ÿçš„æ•°å­¦æ¨¡å‹**:
```scala
// ç‰©ç†å±æ€§çš„æŠ½è±¡å®šä¹‰
abstract class PhysicalProperty {
  def satisfies(requirement: PhysicalProperty): Boolean  // å±æ€§æ»¡è¶³å…³ç³»
  def combine(other: PhysicalProperty): PhysicalProperty // å±æ€§ç»„åˆ
  def cost: Cost                                          // å±æ€§ç›¸å…³æˆæœ¬
}

// å¸¸è§ç‰©ç†å±æ€§çš„å…·ä½“å®ç°
case class SortProperty(ordering: List[SortOrder]) extends PhysicalProperty {
  def satisfies(requirement: PhysicalProperty): Boolean = {
    requirement match {
      case SortProperty(reqOrdering) => 
        reqOrdering.forall(req => ordering.contains(req))
      case _ => false
    }
  }
  
  def combine(other: PhysicalProperty): PhysicalProperty = {
    other match {
      case SortProperty(otherOrdering) =>
        // åˆå¹¶æ’åºè¦æ±‚ï¼Œå–æœ€ä¸¥æ ¼çš„çº¦æŸ
        val combined = mergeSortOrderings(ordering, otherOrdering)
        SortProperty(combined)
      case _ => this
    }
  }
}

case class PartitionProperty(
  partitioningScheme: PartitioningScheme,
  partitionCount: Int
) extends PhysicalProperty {
  def satisfies(requirement: PhysicalProperty): Boolean = {
    requirement match {
      case PartitionProperty(reqScheme, reqCount) =>
        partitioningScheme.compatibleWith(reqScheme) && 
        partitionCount >= reqCount
      case _ => false
    }
  }
}

// å±æ€§æ¨å¯¼è§„åˆ™çš„å½¢å¼åŒ–å®šä¹‰
abstract class PropertyDerivationRule {
  def apply(
    operator: LogicalOperator,
    inputProperties: List[PhysicalProperty]
  ): List[PhysicalProperty]
}

// å…·ä½“çš„å±æ€§æ¨å¯¼è§„åˆ™ç¤ºä¾‹
class SortDerivationRule extends PropertyDerivationRule {
  def apply(
    operator: LogicalOperator,
    inputProperties: List[PhysicalProperty]
  ): List[PhysicalProperty] = {
    operator match {
      case Sort(orderBy) =>
        // æ’åºæ“ä½œç¬¦å¯ä»¥ä¿è¯è¾“å‡ºæœ‰åº
        List(SortProperty(orderBy))
      case Filter(condition) =>
        // è¿‡æ»¤æ“ä½œä¿æŒè¾“å…¥çš„æ’åºå±æ€§
        inputProperties.collect { case sortProp: SortProperty => sortProp }
      case HashJoin(joinKeys, _, _) =>
        // HashJoiné€šå¸¸ä¸ä¿è¯æ’åº
        List.empty
      case _ => List.empty
    }
  }
}
```

#### ğŸ§ª æŸ¥è¯¢ä¼˜åŒ–å™¨çš„å®éªŒéªŒè¯æ–¹æ³•

**ä¼˜åŒ–å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•**:
```scala
// æŸ¥è¯¢ä¼˜åŒ–å™¨çš„ç³»ç»Ÿæ€§è¯„ä¼°
class OptimizerBenchmark {
  // TPC-DSåŸºå‡†æŸ¥è¯¢é›†åˆ
  val tpdsQueries: List[SQLQuery] = loadTPCDSQueries()
  
  // è¯„ä¼°æŒ‡æ ‡
  case class OptimizationMetrics(
    optimizationTime: Duration,        // ä¼˜åŒ–æ—¶é—´
    planQuality: Double,               // è®¡åˆ’è´¨é‡ï¼ˆå®é™…æ‰§è¡Œæ—¶é—´ï¼‰
    searchSpaceSize: Long,             // æœç´¢ç©ºé—´å¤§å°
    memoryUsage: Long,                 // å†…å­˜ä½¿ç”¨é‡
    planStability: Double              // è®¡åˆ’ç¨³å®šæ€§ï¼ˆå¤šæ¬¡è¿è¡Œçš„ä¸€è‡´æ€§ï¼‰
  )
  
  def benchmarkOptimizer(optimizer: QueryOptimizer): OptimizationMetrics = {
    val results = tpdsQueries.map { query =>
      val startTime = System.nanoTime()
      val plan = optimizer.optimize(query)
      val optimizationTime = Duration.fromNanos(System.nanoTime() - startTime)
      
      val executionTime = executePlan(plan)
      val planQuality = executionTime.toMillis
      
      OptimizationMetrics(
        optimizationTime = optimizationTime,
        planQuality = planQuality,
        searchSpaceSize = optimizer.searchSpaceSize,
        memoryUsage = optimizer.memoryUsage,
        planStability = measurePlanStability(optimizer, query)
      )
    }
    
    aggregateMetrics(results)
  }
  
  // è®¡åˆ’ç¨³å®šæ€§æµ‹è¯•ï¼šå¤šæ¬¡è¿è¡Œä¼˜åŒ–å™¨ï¼Œæ£€æŸ¥ç»“æœä¸€è‡´æ€§
  def measurePlanStability(
    optimizer: QueryOptimizer, 
    query: SQLQuery, 
    runs: Int = 10
  ): Double = {
    val plans = (1 to runs).map(_ => optimizer.optimize(query))
    val distinctPlans = plans.distinct.size
    
    // ç¨³å®šæ€§å¾—åˆ†ï¼š1è¡¨ç¤ºå®Œå…¨ç¨³å®šï¼Œ0è¡¨ç¤ºå®Œå…¨ä¸ç¨³å®š
    if (distinctPlans == 1) 1.0 else 1.0 / distinctPlans
  }
}
```

### ğŸ’° 2.2 æˆæœ¬ä¼°ç®—æ¨¡å‹çš„æ•°å­¦åŸºç¡€

#### ğŸ›ï¸ System RåŠ¨æ€è§„åˆ’ç®—æ³•æ·±åº¦è§£æ

**"Access Path Selection in a Relational Database Management System" (Selinger et al., 1979)**
- **å‘è¡¨**: IBM System Ré¡¹ç›®è®ºæ–‡
- **è´¡çŒ®**: å¥ å®šäº†ç°ä»£æŸ¥è¯¢ä¼˜åŒ–æˆæœ¬ä¼°ç®—çš„åŸºç¡€
- **ç†è®ºåˆ›æ–°**: åŠ¨æ€è§„åˆ’ + ç»Ÿè®¡ä¿¡æ¯ + æˆæœ¬æ¨¡å‹

**æˆæœ¬æ¨¡å‹çš„æ•°å­¦å½¢å¼åŒ–**:
```scala
// System Ræˆæœ¬æ¨¡å‹çš„ç²¾ç¡®æ•°å­¦å®šä¹‰
case class Cost(
  ioCost: Double,        // I/Oæˆæœ¬ï¼šç£ç›˜è®¿é—®æ¬¡æ•°
  cpuCost: Double,       // CPUæˆæœ¬ï¼šå¤„ç†æŒ‡ä»¤æ•°
  networkCost: Double,   // ç½‘ç»œæˆæœ¬ï¼šæ•°æ®ä¼ è¾“é‡
  memoryCost: Double     // å†…å­˜æˆæœ¬ï¼šå†…å­˜ä½¿ç”¨é‡
) {
  def total: Double = ioCost + cpuCost + networkCost + memoryCost
  
  // æˆæœ¬çš„åŠ æƒç»„åˆï¼ˆå¯æ ¹æ®ç¡¬ä»¶ç‰¹æ€§è°ƒæ•´ï¼‰
  def weighted(weights: CostWeights): Double = {
    ioCost * weights.ioWeight +
    cpuCost * weights.cpuWeight +
    networkCost * weights.networkWeight +
    memoryCost * weights.memoryWeight
  }
}

case class CostWeights(
  ioWeight: Double = 1.0,
  cpuWeight: Double = 0.1,
  networkWeight: Double = 10.0,
  memoryWeight: Double = 0.01
)

// é€‰æ‹©æ€§ä¼°ç®—çš„æ•°å­¦æ¨¡å‹
class SelectivityEstimator {
  // åŸºäºç»Ÿè®¡ä¿¡æ¯çš„é€‰æ‹©æ€§è®¡ç®—
  def estimateSelectivity(
    predicate: Predicate, 
    statistics: TableStatistics
  ): Double = {
    predicate match {
      case Equals(column, value) =>
        // ç­‰å€¼è°“è¯ï¼š1/NDV (Number of Distinct Values)
        1.0 / statistics.columnDistinctCount(column)
        
      case Range(column, min, max) =>
        // èŒƒå›´è°“è¯ï¼š(max-min)/column_range
        val columnRange = statistics.columnRange(column)
        (max - min) / columnRange
        
      case Like(column, pattern) =>
        // LIKEè°“è¯ï¼šåŸºäºå¯å‘å¼ä¼°ç®—
        estimateLikeSelectivity(pattern)
        
      case And(predicates) =>
        // ANDè°“è¯ï¼šé€‰æ‹©æ€§çš„ä¹˜ç§¯ï¼ˆå‡è®¾ç‹¬ç«‹æ€§ï¼‰
        predicates.map(estimateSelectivity(_, statistics)).product
        
      case Or(predicates) =>
        // ORè°“è¯ï¼šä½¿ç”¨å®¹æ–¥åŸç†
        estimateOrSelectivity(predicates, statistics)
    }
  }
  
  // ç›´æ–¹å›¾è¾…åŠ©çš„é€‰æ‹©æ€§ä¼°ç®—
  def estimateWithHistogram(
    predicate: Predicate,
    histogram: Histogram
  ): Double = {
    histogram match {
      case EquiWidthHistogram(buckets, min, max) =>
        estimateEquiWidthSelectivity(predicate, buckets, min, max)
        case EquiHeightHistogram(buckets) =>
        estimateEquiHeightSelectivity(predicate, buckets)
    }
  }
}
```

**åŠ¨æ€è§„åˆ’ä¼˜åŒ–ç®—æ³•çš„æ•°å­¦åˆ†æ**:
```scala
// System RåŠ¨æ€è§„åˆ’ç®—æ³•çš„ç²¾ç¡®å®ç°
class SystemRDynamicProgramming {
  // è®°å¿†åŒ–è¡¨ï¼šå­˜å‚¨å­é—®é¢˜çš„æœ€ä¼˜è§£
  val memoTable: mutable.Map[Set[Relation], Plan] = mutable.Map()
  
  // ä¸»ä¼˜åŒ–å‡½æ•°ï¼šé€’å½’åŠ¨æ€è§„åˆ’
  def optimizeJoin(relations: Set[Relation]): Plan = {
    // åŸºç¡€æƒ…å†µï¼šå•å…³ç³»ç›´æ¥æ‰«æ
    if (relations.size == 1) {
      return createScanPlan(relations.head)
    }
    
    // æ£€æŸ¥è®°å¿†åŒ–è¡¨
    memoTable.get(relations) match {
      case Some(plan) => return plan
      case None => // ç»§ç»­è®¡ç®—
    }
    
    // é€’å½’æƒ…å†µï¼šæšä¸¾æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²
    val bestPlan = enumerateJoinOrders(relations).minBy(_.cost)
    memoTable += (relations -> bestPlan)
    bestPlan
  }
  
  // æšä¸¾æ‰€æœ‰å¯èƒ½çš„è¿æ¥é¡ºåºï¼ˆåŸºäºåŠ¨æ€è§„åˆ’ï¼‰
  def enumerateJoinOrders(relations: Set[Relation]): List[Plan] = {
    val plans = mutable.ListBuffer[Plan]()
    
    // å¯¹æ‰€æœ‰å¯èƒ½çš„éç©ºçœŸå­é›†è¿›è¡Œåˆ†å‰²
    for (size <- 1 until relations.size) {
      for (subset <- relations.subsets(size)) {
        val complement = relations -- subset
        
        // é€’å½’ä¼˜åŒ–å­é›†
        val leftPlan = optimizeJoin(subset)
        val rightPlan = optimizeJoin(complement)
        
        // ç”Ÿæˆè¿æ¥è®¡åˆ’
        val joinPlans = generateJoinPlans(leftPlan, rightPlan)
        plans ++= joinPlans
      }
    }
    
    plans.toList
  }
  
  // æ—¶é—´å¤æ‚åº¦åˆ†æï¼šO(2^n Ã— n^2)
  // ç©ºé—´å¤æ‚åº¦ï¼šO(2^n)
  // å…¶ä¸­næ˜¯å…³ç³»çš„æ•°é‡
}
```

#### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ç†è®ºçš„é«˜çº§ä¸»é¢˜

**å¤šç»´ç›´æ–¹å›¾ç†è®º**:
```scala
// å¤šç»´ç›´æ–¹å›¾çš„æ•°å­¦æ¨¡å‹
abstract class MultiDimensionalHistogram {
  def dimensions: Int
  def bucketCount: Int
  def estimateQuery(query: MultiDimensionalQuery): Double
}

// MHIST (Multi-dimensional Histogram) ç®—æ³•
class MHistHistogram(
  buckets: List[MHistBucket],
  dimensionCount: Int
) extends MultiDimensionalHistogram {
  
  def estimateQuery(query: MultiDimensionalQuery): Double = {
    // åŸºäºçŸ©çš„ä¼°ç®—æ–¹æ³•
    val relevantBuckets = buckets.filter(bucket => 
      bucket.overlaps(query.range)
    )
    
    relevantBuckets.map { bucket =>
      val overlapVolume = bucket.overlapVolume(query.range)
      val bucketVolume = bucket.volume
      val selectivity = overlapVolume / bucketVolume
      selectivity * bucket.frequency
    }.sum
  }
}

case class MHistBucket(
  ranges: List[Range],           // æ¯ä¸ªç»´åº¦çš„èŒƒå›´
  frequency: Double,             // é¢‘ç‡
  moments: List[Double]          // å„é˜¶çŸ©ï¼ˆç”¨äºæ›´ç²¾ç¡®çš„ä¼°ç®—ï¼‰
) {
  def volume: Double = ranges.map(_.length).product
  
  def overlaps(queryRange: List[Range]): Boolean = {
    ranges.zip(queryRange).forall { case (bucketRange, queryRange) =>
      bucketRange.intersects(queryRange)
    }
  }
  
  def overlapVolume(queryRange: List[Range]): Double = {
    ranges.zip(queryRange).map { case (bucketRange, queryRange) =>
      bucketRange.intersection(queryRange).length
    }.product
  }
}
```

**é‡‡æ ·ä¸ç»Ÿè®¡æ¨æ–­ç†è®º**:
```scala
// åŸºäºé‡‡æ ·çš„ç»Ÿè®¡ä¿¡æ¯ä¼°ç®—
class SamplingStatistics {
  // ä¼¯åŠªåˆ©é‡‡æ ·æ¨¡å‹
case class BernoulliSample(
  sampleRate: Double,
  sampleSize: Long,
  populationSize: Long
) {
  // ä¼°ç®—æ€»ä½“ç»Ÿè®¡é‡
  def estimatePopulationVariance(sampleVariance: Double): Double = {
    // ä½¿ç”¨æœ‰é™æ€»ä½“ä¿®æ­£
    val correctionFactor = (populationSize - sampleSize) / (populationSize - 1)
    sampleVariance / sampleRate * correctionFactor
  }
  
  // ç½®ä¿¡åŒºé—´è®¡ç®—
  def confidenceInterval(
    sampleMean: Double, 
    sampleVariance: Double,
    confidenceLevel: Double
  ): (Double, Double) = {
    val zScore = normalQuantile(1 - (1 - confidenceLevel) / 2)
    val standardError = math.sqrt(sampleVariance / sampleSize)
    val margin = zScore * standardError
    
    (sampleMean - margin, sampleMean + margin)
  }
}

// åˆ†å±‚é‡‡æ ·ç†è®º
class StratifiedSampling[T](
  strata: Map[String, List[T]],
  stratumWeights: Map[String, Double]
) {
  def estimateOverallMean(
    stratumMeans: Map[String, Double],
    stratumVariances: Map[String, Double],
    stratumSampleSizes: Map[String, Int]
  ): (Double, Double) = {
    // æ€»ä½“å‡å€¼ä¼°è®¡
    val overallMean = stratumMeans.map { case (stratum, mean) =>
      mean * stratumWeights(stratum)
    }.sum
    
    // ä¼°è®¡æ–¹å·®
    val variance = stratumVariances.map { case (stratum, variance) =>
      val weight = stratumWeights(stratum)
      val sampleSize = stratumSampleSizes(stratum)
      variance * weight * weight / sampleSize
    }.sum
    
    (overallMean, variance)
  }
}
```

### ğŸš€ 2.3 å‘é‡åŒ–æ‰§è¡Œç†è®ºçš„æ•°å­¦åŸºç¡€

#### ğŸ’» SIMDæŒ‡ä»¤ç†è®ºçš„æ·±åº¦åˆ†æ

**å‘é‡åŒ–åŸç†çš„æ•°å­¦å»ºæ¨¡**:
```scala
// SIMD (Single Instruction, Multiple Data) çš„å½¢å¼åŒ–æ¨¡å‹
abstract class SIMDVector[T] {
  def width: Int                    // å‘é‡å®½åº¦ï¼ˆå…ƒç´ ä¸ªæ•°ï¼‰
  def elements: Array[T]           // å‘é‡å…ƒç´ 
  
  // å‘é‡æ“ä½œçš„æ•°å­¦å®šä¹‰
  def add(other: SIMDVector[T]): SIMDVector[T]
  def multiply(other: SIMDVector[T]): SIMDVector[T]
  def compare(other: SIMDVector[T]): SIMDVector[Boolean]
  def select(mask: SIMDVector[Boolean], trueValues: SIMDVector[T]): SIMDVector[T]
}

// å…·ä½“çš„SIMDå®ç°ï¼ˆä»¥AVX-512ä¸ºä¾‹ï¼‰
class AVX512IntVector(elements: Array[Int]) extends SIMDVector[Int] {
  val width: Int = 16  // AVX-512å¯ä»¥å¤„ç†16ä¸ª32ä½æ•´æ•°
  
  def add(other: SIMDVector[Int]): SIMDVector[Int] = {
    // ä½¿ç”¨AVX-512çš„VPADDDæŒ‡ä»¤
    new AVX512IntVector(
      elements.zip(other.elements).map { case (a, b) => a + b }
    )
  }
  
  // å‘é‡åŒ–çš„è°“è¯è¯„ä¼°
  def evaluatePredicate(predicate: Int => Boolean): SIMDVector[Boolean] = {
    val mask = elements.map(predicate)
    new AVX512BoolVector(mask)
  }
}

// å‘é‡åŒ–æ‰§è¡Œå¼•æ“çš„ç†è®ºæ¨¡å‹
class VectorizedExecutionEngine {
  // æ‰¹å¤„ç†çš„å¤§å°ï¼ˆé€šå¸¸ä¸CPUç¼“å­˜è¡Œå¯¹é½ï¼‰
  val batchSize: Int = 1024
  
  // å‘é‡åŒ–è¿‡æ»¤æ“ä½œ
  def vectorizedFilter(
    input: VectorizedBatch,
    predicate: Column => SIMDVector[Boolean]
  ): VectorizedBatch = {
    val selectionVectors = input.columns.map(predicate)
    val combinedMask = combineMasks(selectionVectors)
    
    // ä½¿ç”¨å‘é‡åŒ–çš„gatheræ“ä½œ
    input.filterByMask(combinedMask)
  }
  
  // å‘é‡åŒ–èšåˆæ“ä½œ
  def vectorizedAggregate(
    input: VectorizedBatch,
    groupByColumns: List[String],
    aggregateFunctions: List[AggregateFunction]
  ): VectorizedBatch = {
    // ä½¿ç”¨SIMDæŒ‡ä»¤è¿›è¡Œå¹¶è¡Œèšåˆ
    val groups = groupBy(input, groupByColumns)
    
    groups.map { case (groupKey, groupBatch) =>
      val aggregates = aggregateFunctions.map(_.applyVectorized(groupBatch))
      createResultBatch(groupKey, aggregates)
    }.reduce(mergeBatches)
  }
}
```

#### ğŸ¯ Apache Arrowå†…å­˜å¸ƒå±€çš„æ•°å­¦åˆ†æ

**Arrowå†…å­˜å¸ƒå±€çš„å½¢å¼åŒ–å®šä¹‰**:
```scala
// Arrowæ•°ç»„çš„é€šç”¨å†…å­˜å¸ƒå±€æ¨¡å‹
abstract class ArrowArray {
  def buffers: List[ByteBuffer]     // ç¼“å†²åŒºåˆ—è¡¨
  def length: Int                   // å…ƒç´ ä¸ªæ•°
  def nullCount: Int                // ç©ºå€¼ä¸ªæ•°
  def offset: Int                   // åç§»é‡
  
  // å†…å­˜å¸ƒå±€çš„æ•°å­¦æè¿°
  def memoryLayout: MemoryLayout
}

// åŸºç¡€ç±»å‹æ•°ç»„çš„å†…å­˜å¸ƒå±€
class PrimitiveArray[T](
  buffers: List[ByteBuffer],
  length: Int,
  nullCount: Int,
  offset: Int
) extends ArrowArray {
  
  // ç¼“å†²åŒºç»“æ„ï¼š
  // Buffer 0: Validity bitmap (ç©ºå€¼ä½å›¾)
  // Buffer 1: Data buffer (å®é™…æ•°æ®)
  def memoryLayout: MemoryLayout = {
    val validityBitmapSize = math.ceil(length / 8.0).toInt
    val dataBufferSize = length * sizeOf[T]()
    
    MemoryLayout(
      buffers = List(
        BufferInfo(0, validityBitmapSize, "validity bitmap"),
        BufferInfo(validityBitmapSize, dataBufferSize, "data buffer")
      ),
      totalSize = validityBitmapSize + dataBufferSize
    )
  }
}

// å˜é•¿å­—ç¬¦ä¸²æ•°ç»„çš„å†…å­˜å¸ƒå±€
class StringArray(
  buffers: List[ByteBuffer],
  length: Int,
  nullCount: Int,
  offset: Int
) extends ArrowArray {
  
  // ç¼“å†²åŒºç»“æ„ï¼š
  // Buffer 0: Validity bitmap
  // Buffer 1: Offset buffer (æ¯ä¸ªå­—ç¬¦ä¸²çš„èµ·å§‹åç§»)
  // Buffer 2: Data buffer (å®é™…å­—ç¬¦ä¸²æ•°æ®)
  def memoryLayout: MemoryLayout = {
    val validityBitmapSize = math.ceil(length / 8.0).toInt
    val offsetBufferSize = (length + 1) * 4  // æ¯ä¸ªåç§»4å­—èŠ‚
    val dataBufferSize = buffers(2).remaining()
    
    MemoryLayout(
      buffers = List(
        BufferInfo(0, validityBitmapSize, "validity bitmap"),
        BufferInfo(validityBitmapSize, offsetBufferSize, "offset buffer"),
        BufferInfo(validityBitmapSize + offsetBufferSize, dataBufferSize, "data buffer")
      ),
      totalSize = validityBitmapSize + offsetBufferSize + dataBufferSize
    )
  }
}

// é›¶æ‹·è´ä¼ è¾“çš„æ•°å­¦ä¿è¯
class ZeroCopyTransmission {
  // é›¶æ‹·è´çš„æ¡ä»¶ï¼šå†…å­˜å¸ƒå±€å®Œå…¨å…¼å®¹
  def isZeroCopyCompatible(array1: ArrowArray, array2: ArrowArray): Boolean = {
    array1.memoryLayout == array2.memoryLayout &&
    array1.length == array2.length &&
    array1.offset == array2.offset
  }
  
  // é›¶æ‹·è´åºåˆ—åŒ–ï¼šç›´æ¥è¿”å›å†…å­˜æ˜ å°„
  def serializeZeroCopy(array: ArrowArray): ByteBuffer = {
    // åˆ›å»ºä¸€ä¸ªè¦†ç›–æ‰€æœ‰ç¼“å†²åŒºçš„è§†å›¾
    val totalBuffer = array.buffers.head.duplicate()
    totalBuffer.limit(array.memoryLayout.totalSize)
    totalBuffer
  }
  
  // é›¶æ‹·è´ååºåˆ—åŒ–ï¼šç›´æ¥ä½¿ç”¨å†…å­˜æ˜ å°„
  def deserializeZeroCopy(buffer: ByteBuffer, schema: ArrowSchema): ArrowArray = {
    // æ— éœ€æ•°æ®å¤åˆ¶ï¼Œç›´æ¥åˆ›å»ºæ•°ç»„è§†å›¾
    ArrowArray.fromBuffer(buffer, schema)
  }
}
```

#### ğŸ§ª å‘é‡åŒ–æ‰§è¡Œçš„æ€§èƒ½å»ºæ¨¡

**å‘é‡åŒ–æ‰§è¡Œçš„ç†è®ºæ€§èƒ½åˆ†æ**:
```scala
// å‘é‡åŒ–æ‰§è¡Œçš„æ€§èƒ½æ¨¡å‹
class VectorizedPerformanceModel {
  // æ€§èƒ½å‚æ•°
case class PerformanceParameters(
  cpuFrequency: Double,           // CPUé¢‘ç‡ (GHz)
  vectorWidth: Int,               // å‘é‡å®½åº¦
  memoryBandwidth: Double,        // å†…å­˜å¸¦å®½ (GB/s)
  cacheSizes: Map[String, Long]   // å„çº§ç¼“å­˜å¤§å°
)
  
  // æ‰§è¡Œæ—¶é—´çš„ç†è®ºè®¡ç®—
  def estimateExecutionTime(
    operation: VectorizedOperation,
    dataSize: Long,
    params: PerformanceParameters
  ): Duration = {
    operation match {
      case VectorizedFilter(_) =>
        // è¿‡æ»¤æ“ä½œï¼šä¸»è¦å—å†…å­˜å¸¦å®½é™åˆ¶
        val memoryAccessTime = dataSize / (params.memoryBandwidth * 1e9)
        val computationTime = dataSize / (params.vectorWidth * params.cpuFrequency * 1e9)
        Duration.ofNanos((memoryAccessTime + computationTime * 1e9).toLong)
        
      case VectorizedAggregate(_) =>
        // èšåˆæ“ä½œï¼šè®¡ç®—å¯†é›†å‹
        val computationTime = dataSize / (params.vectorWidth * params.cpuFrequency * 1e9)
        Duration.ofNanos((computationTime * 1e9).toLong)
        
      case VectorizedJoin(_) =>
        // è¿æ¥æ“ä½œï¼šå¤æ‚çš„å¤šé˜¶æ®µè¿‡ç¨‹
        estimateJoinTime(dataSize, params)
    }
  }
  
  // ç¼“å­˜æ€§èƒ½å»ºæ¨¡
  def estimateCachePerformance(
    accessPattern: AccessPattern,
    cacheSize: Long,
    dataSize: Long
  ): CachePerformance = {
    val workingSetSize = accessPattern.workingSetSize
    
    if (workingSetSize <= cacheSize) {
      // å…¨éƒ¨å‘½ä¸­ç¼“å­˜
      CachePerformance(hitRate = 1.0, averageLatency = 1.0) // çº³ç§’çº§
    } else {
      // éƒ¨åˆ†å‘½ä¸­ç¼“å­˜ï¼ˆåŸºäºLRUæ¨¡å‹çš„ä¼°ç®—ï¼‰
      val hitRate = cacheSize / workingSetSize
      val averageLatency = hitRate * 1.0 + (1 - hitRate) * 100.0 // ç¼“å­˜1nsï¼Œå†…å­˜100ns
      CachePerformance(hitRate, averageLatency)
    }
  }
}

// å‘é‡åŒ– vs æ ‡é‡æ‰§è¡Œçš„æ€§èƒ½å¯¹æ¯”
class VectorizedVsScalarComparison {
  def performanceRatio(
    vectorizedTime: Duration,
    scalarTime: Duration
  ): Double = scalarTime.toMillis.toDouble / vectorizedTime.toMillis
  
  def theoreticalSpeedup(
    vectorWidth: Int,
    operationType: OperationType
  ): Double = {
    operationType match {
      case EmbarrassinglyParallel => vectorWidth  // å®Œå…¨å¹¶è¡Œ
      case MemoryBound => math.sqrt(vectorWidth)  // å†…å­˜é™åˆ¶
      case ComputeBound => vectorWidth * 0.8      // è®¡ç®—é™åˆ¶ï¼ˆè€ƒè™‘å¼€é”€ï¼‰
    }
  }
}
```
---

## ğŸ’¾ 3. å†…å­˜è®¡ç®—ä¸é›¶æ‹·è´ç†è®º

### ğŸ›ï¸ 3.1 åˆ—å¼å­˜å‚¨ç†è®ºçš„æ•°å­¦åŸºç¡€

#### ğŸ“Š C-Storeè®¾è®¡åŸç†çš„æ·±åº¦è§£æ

**"C-Store: A Column-Oriented DBMS" (Stonebraker et al., 2005)**
- **å‘è¡¨**: VLDB 2005
- **è´¡çŒ®**: å¥ å®šäº†ç°ä»£åˆ—å¼æ•°æ®åº“çš„ç†è®ºåŸºç¡€
- **ç†è®ºåˆ›æ–°**: åˆ—å¼å­˜å‚¨ + å‹ç¼©ä¼˜åŒ– + å†™ä¼˜åŒ–å­˜å‚¨

**åˆ—å¼å­˜å‚¨çš„æ•°å­¦æ¨¡å‹**:
```scala
// åˆ—å¼å­˜å‚¨çš„å½¢å¼åŒ–å®šä¹‰
case class ColumnarTable(
  columns: Map[String, Column],
  rowCount: Long,
  compressionSchemes: Map[String, CompressionScheme]
) {
  // åˆ—å¼å­˜å‚¨çš„ä¼˜åŠ¿åˆ†æ
  def storageEfficiency: Double = {
    val rowStoreSize = calculateRowStoreSize()
    val columnStoreSize = calculateColumnStoreSize()
    rowStoreSize / columnStoreSize
  }
  
  // æŸ¥è¯¢æ•ˆç‡çš„æ•°å­¦å»ºæ¨¡
  def queryEfficiency(queryColumns: Set[String]): Double = {
    val accessedColumns = queryColumns.size
    val totalColumns = columns.size
    
    // åˆ—å¼å­˜å‚¨åªè¯»å–éœ€è¦çš„åˆ—
    val ioReduction = totalColumns.toDouble / accessedColumns
    
    // è€ƒè™‘å‹ç¼©çš„é¢å¤–æ”¶ç›Š
    val compressionBenefit = queryColumns.map { col =>
      compressionSchemes.get(col).map(_.compressionRatio).getOrElse(1.0)
    }.product
    
    ioReduction * compressionBenefit
  }
}

// åˆ—çš„æ•°å­¦è¡¨ç¤º
case class Column(
  name: String,
  dataType: DataType,
  values: Array[Any],
  nullBitmap: BitSet,
  statistics: ColumnStatistics
) {
  // åˆ—çº§çš„ç»Ÿè®¡ä¿¡æ¯
  def updateStatistics(): ColumnStatistics = {
    val nonNullValues = values.zip(nullBitmap).filter(_._2).map(_._1)
    
    ColumnStatistics(
      distinctCount = nonNullValues.distinct.size,
      nullCount = values.size - nonNullValues.size,
      minValue = nonNullValues.min,
      maxValue = nonNullValues.max,
      averageValue = nonNullValues.sum / nonNullValues.size
    )
  }
}
```

**å‹ç¼©ç®—æ³•çš„æ•°å­¦åˆ†æ**:
```scala
// åˆ—å¼å‹ç¼©çš„ç†è®ºåŸºç¡€
abstract class CompressionScheme {
  def compressionRatio: Double  // å‹ç¼©æ¯”
  def compressionCost: Double   // å‹ç¼©å¼€é”€
  def decompressionCost: Double // è§£å‹å¼€é”€
  
  def compress(data: Array[Any]): Array[Byte]
  def decompress(compressedData: Array[Byte]): Array[Any]
}

// å­—å…¸ç¼–ç çš„æ•°å­¦æ¨¡å‹
class DictionaryEncoding extends CompressionScheme {
  def compressionRatio: Double = {
    // å­—å…¸ç¼–ç çš„å‹ç¼©æ¯”å–å†³äºåŸºæ•°
    // Ratio = (original_size) / (dictionary_size + encoded_size)
  }
  
  def compress(data: Array[Any]): Array[Byte] = {
    // 1. æ„å»ºå­—å…¸
    val dictionary = data.distinct.zipWithIndex.toMap
    
    // 2. ç¼–ç æ•°æ®
    val encodedData = data.map(dictionary)
    
    // 3. åºåˆ—åŒ–å­—å…¸å’Œç¼–ç æ•°æ®
    serializeDictionaryAndData(dictionary, encodedData)
  }
  
  // å­—å…¸ç¼–ç çš„æœ€ä¼˜æ€§åˆ†æ
  def analyzeOptimality(data: Array[Any]): CompressionAnalysis = {
    val cardinality = data.distinct.size.toDouble
    val dataSize = data.size.toDouble
    
    // æœ€ä¼˜å‹ç¼©æ¯”çš„ç†è®ºä¸Šç•Œ
    val theoreticalOptimal = math.log2(cardinality) / math.log2(dataSize)
    
    // å®é™…å‹ç¼©æ¯”
    val actualRatio = calculateActualRatio(data)
    
    CompressionAnalysis(
      theoreticalOptimal = theoreticalOptimal,
      actualRatio = actualRatio,
      efficiency = actualRatio / theoreticalOptimal
    )
  }
}

// æ¸¸ç¨‹ç¼–ç çš„æ•°å­¦æ¨¡å‹
class RunLengthEncoding extends CompressionScheme {
  def compress(data: Array[Any]): Array[Byte] = {
    val runs = encodeRuns(data)
    serializeRuns(runs)
  }
  
  // æ¸¸ç¨‹ç¼–ç çš„å‹ç¼©æ¯”ç†è®ºåˆ†æ
  def theoreticalCompressionRatio(
    averageRunLength: Double,
    symbolSize: Int
  ): Double = {
    // RLEå‹ç¼©æ¯” = (original_size) / (runs * (symbol_size + count_size))
    val originalSize = averageRunLength * symbolSize
    val compressedSize = symbolSize + 4 // å‡è®¾countä¸º4å­—èŠ‚
    originalSize / compressedSize
  }
}
```

#### ğŸ§® MonetDBçš„å‘é‡åŒ–æŸ¥è¯¢æ‰§è¡Œç†è®º

**MonetDBçš„å‘é‡åŒ–æ‰§è¡Œæ¨¡å‹**:
```scala
// MonetDBçš„MIL (MonetDB Interface Language) ç†è®ºåŸºç¡€
class MonetDBVectorizedExecution {
  // BAT (Binary Association Table) çš„æ•°å­¦å®šä¹‰
case class BAT[
  HeadType, 
  TailType
](
  head: Column[HeadType],   // å¤´åˆ—
  tail: Column[TailType]    // å°¾åˆ—
) {
  // BATçš„ä»£æ•°æ“ä½œ
  def join[OtherType](other: BAT[TailType, OtherType]): BAT[HeadType, OtherType] = {
    // åŸºäºå“ˆå¸Œçš„è¿æ¥æ“ä½œ
    val hashTable = buildHashTable(tail.values)
    val matches = other.tail.values.flatMap(hashTable.get)
    
    BAT(
      head = Column(head.values.filter(matches.contains)),
      tail = Column(other.head.values.filter(matches.contains))
    )
  }
  
  // é€‰æ‹©æ“ä½œçš„å‘é‡åŒ–å®ç°
  def select(predicate: TailType => Boolean): BAT[HeadType, TailType] = {
    val mask = tail.values.map(predicate)
    BAT(
      head = Column(head.values.zip(mask).filter(_._2).map(_._1)),
      tail = Column(tail.values.zip(mask).filter(_._2).map(_._1))
    )
  }
}

// å‘é‡åŒ–æŸ¥è¯¢æ‰§è¡Œçš„ç†è®ºæ¨¡å‹
class VectorizedQueryProcessor {
  // æ‰¹å¤„ç†çš„å¤§å°ä¼˜åŒ–
  def optimalBatchSize(
    cacheSize: Long,
    tupleSize: Int,
    selectivity: Double
  ): Int = {
    // ç›®æ ‡ï¼šæœ€å¤§åŒ–ç¼“å­˜åˆ©ç”¨ç‡
    val workingSetSize = tupleSize * 1024 // åˆå§‹æ‰¹å¤§å°
    
    if (workingSetSize <= cacheSize) {
      // å¦‚æœå·¥ä½œé›†èƒ½æ”¾å…¥ç¼“å­˜ï¼Œå¢åŠ æ‰¹å¤§å°
      (cacheSize / tupleSize).toInt
    } else {
      // å¦åˆ™å‡å°‘æ‰¹å¤§å°ä»¥é€‚åº”ç¼“å­˜
      (cacheSize / tupleSize / selectivity).toInt
    }
  }
  
  // å‘é‡åŒ–æ“ä½œçš„æ€§èƒ½å»ºæ¨¡
  def vectorizedPerformance(
    operation: VectorizedOperation,
    batchSize: Int,
    vectorWidth: Int
  ): PerformanceMetrics = {
    val vectorizedBatches = math.ceil(batchSize / vectorWidth.toDouble).toInt
    
    PerformanceMetrics(
      cpuCycles = vectorizedBatches * operation.vectorCost,
      memoryAccesses = batchSize * operation.memoryCost,
      cacheMisses = estimateCacheMisses(batchSize, operation.workingSetSize)
    )
  }
}
```

### ğŸ—ï¸ 3.2 NUMAæ¶æ„ä¼˜åŒ–çš„ç†è®ºåˆ†æ

#### ğŸ§  NUMAå†…å­˜å±‚æ¬¡ç»“æ„çš„æ•°å­¦å»ºæ¨¡

**NUMAæ¶æ„çš„å½¢å¼åŒ–å®šä¹‰**:
```scala
// NUMAç³»ç»Ÿçš„æ•°å­¦æ¨¡å‹
case class NUMAArchitecture(
  nodes: List[NUMANode],
  interconnectLatency: Map[(NodeId, NodeId), Latency],
  interconnectBandwidth: Map[(NodeId, NodeId), Bandwidth]
) {
  // å†…å­˜è®¿é—®å»¶è¿Ÿçš„æ•°å­¦è®¡ç®—
  def memoryAccessLatency(sourceNode: NodeId, targetNode: NodeId): Latency = {
    if (sourceNode == targetNode) {
      // æœ¬åœ°å†…å­˜è®¿é—®
      nodes(sourceNode).localMemoryLatency
    } else {
      // è¿œç¨‹å†…å­˜è®¿é—®
      val localLatency = nodes(sourceNode).localMemoryLatency
      val remoteLatency = nodes(targetNode).localMemoryLatency
      val interconnectLat = interconnectLatency((sourceNode, targetNode))
      
      localLatency + remoteLatency + interconnectLat
    }
  }
  
  // NUMAæ•ˆåº”çš„é‡åŒ–åˆ†æ
  def numaEffect(): NUMAEffect = {
    val localAccesses = measureLocalAccesses()
    val remoteAccesses = measureRemoteAccesses()
    
    val localLatency = averageLocalLatency()
    val remoteLatency = averageRemoteLatency()
    
    val numaRatio = remoteLatency / localLatency
    val localityRatio = localAccesses.toDouble / (localAccesses + remoteAccesses)
    
    NUMAEffect(
      numaRatio = numaRatio,
      localityRatio = localityRatio,
      performanceImpact = 1.0 / (localityRatio + (1 - localityRatio) * numaRatio)
    )
  }
}

case class NUMANode(
  id: NodeId,
  cpus: List[CPU],
  memorySize: Long,
  localMemoryLatency: Latency,
  memoryBandwidth: Bandwidth
)

// NUMAæ„ŸçŸ¥çš„æ•°æ®å¸ƒå±€ç®—æ³•
class NUMAAwareDataLayout {
  def optimizeDataPlacement(
    data: DistributedData,
    accessPattern: AccessPattern,
    numaArch: NUMAArchitecture
  ): DataPlacement = {
    // åŸºäºè®¿é—®æ¨¡å¼çš„æ•°æ®æ”¾ç½®ç­–ç•¥
    val nodeAffinity = calculateNodeAffinity(data, accessPattern)
    
    // ä½¿ç”¨å›¾ç€è‰²ç®—æ³•è¿›è¡Œæ•°æ®åˆ†é…
    val placement = graphColoringPlacement(nodeAffinity, numaArch)
    
    // ä¼˜åŒ–è·¨èŠ‚ç‚¹è®¿é—®
    optimizeCrossNodeAccess(placement, numaArch)
  }
  
  // æ•°æ®äº²å’Œæ€§çš„è®¡ç®—
  def calculateNodeAffinity(
    data: DistributedData,
    accessPattern: AccessPattern
  ): Map[DataChunk, Map[NodeId, Double]] = {
    data.chunks.map { chunk =>
      val affinity = accessPattern.accesses
        .filter(_.dataChunk == chunk)
        .groupBy(_.sourceNode)
        .map { case (node, accesses) =>
          node -> accesses.map(_.frequency).sum
        }
      
      chunk -> affinity
    }.toMap
  }
}
```

#### âš¡ NUMAæ„ŸçŸ¥çš„è°ƒåº¦ç®—æ³•ç†è®º

**NUMAæ„ŸçŸ¥ä»»åŠ¡è°ƒåº¦çš„æ•°å­¦æ¨¡å‹**:
```scala
// NUMAæ„ŸçŸ¥çš„è°ƒåº¦å™¨
class NUMAAwareScheduler {
  // ä»»åŠ¡è°ƒåº¦çš„ä¼˜åŒ–ç›®æ ‡å‡½æ•°
  def optimizationObjective(
    schedule: Schedule,
    numaArch: NUMAArchitecture
  ): Double = {
    // 1. æœ€å°åŒ–è¿œç¨‹å†…å­˜è®¿é—®
    val remoteAccessCost = calculateRemoteAccessCost(schedule, numaArch)
    
    // 2. æœ€å¤§åŒ–è´Ÿè½½å‡è¡¡
    val loadBalanceScore = calculateLoadBalance(schedule)
    
    // 3. æœ€å°åŒ–é€šä¿¡å¼€é”€
    val communicationCost = calculateCommunicationCost(schedule, numaArch)
    
    // ç»¼åˆç›®æ ‡å‡½æ•°ï¼ˆæƒé‡å¯è°ƒï¼‰
    0.5 * remoteAccessCost + 0.3 * loadBalanceScore + 0.2 * communicationCost
  }
  
  // åŸºäºè´ªå¿ƒç®—æ³•çš„NUMAæ„ŸçŸ¥è°ƒåº¦
  def scheduleTasksNUMAAware(
    tasks: List[Task],
    numaArch: NUMAArchitecture
  ): Schedule = {
    val unscheduledTasks = tasks.sortBy(-_.priority) // æŒ‰ä¼˜å…ˆçº§æ’åº
    val schedule = mutable.Map[NodeId, List[Task]]()
    
    unscheduledTasks.foreach { task =>
      val bestNode = findBestNUMANode(task, schedule, numaArch)
      schedule(bestNode) = task :: schedule.getOrElse(bestNode, List.empty)
    }
    
    Schedule(schedule.toMap)
  }
  
  // æœ€ä¼˜NUMAèŠ‚ç‚¹çš„é€‰æ‹©ç®—æ³•
  def findBestNUMANode(
    task: Task,
    currentSchedule: Map[NodeId, List[Task]],
    numaArch: NUMAArchitecture
  ): NodeId = {
    val candidateNodes = numaArch.nodes.map(_.id)
    
    candidateNodes.map { nodeId =>
      val dataLocalityScore = calculateDataLocality(task, nodeId)
      val loadBalanceScore = calculateLoadBalanceScore(currentSchedule, nodeId)
      val communicationScore = calculateCommunicationScore(task, nodeId, currentSchedule)
      
      val totalScore = 0.4 * dataLocalityScore + 0.4 * loadBalanceScore + 0.2 * communicationScore
      
      nodeId -> totalScore
    }.maxBy(_._2)._1
  }
}
```

### ğŸŒ 3.3 é«˜æ•ˆæ•°æ®ä¼ è¾“ç†è®ºçš„æ·±åº¦åˆ†æ

#### ğŸš€ RDMAæŠ€æœ¯çš„æ•°å­¦åŸç†

**RDMA (Remote Direct Memory Access) çš„ç†è®ºåŸºç¡€**:
```scala
// RDMAæ“ä½œçš„å½¢å¼åŒ–å®šä¹‰
abstract class RDMAOperation {
  def sourceAddress: MemoryAddress
  def destinationAddress: MemoryAddress
  def dataSize: Long
  def completionTime: Duration
}

case class RDMARead(
  sourceAddress: MemoryAddress,
  destinationAddress: MemoryAddress,
  dataSize: Long,
  rkey: RemoteKey
) extends RDMAOperation {
  def completionTime: Duration = {
    // RDMAè¯»æ“ä½œçš„å»¶è¿Ÿæ¨¡å‹
    val baseLatency = 1.5 // å¾®ç§’çº§åŸºç¡€å»¶è¿Ÿ
    val bandwidthLatency = dataSize / networkBandwidth
    val processingLatency = dataSize / processingRate
    
    Duration.ofNanos((baseLatency + bandwidthLatency + processingLatency) * 1000)
  }
}

case class RDMAWrite(
  sourceAddress: MemoryAddress,
  destinationAddress: MemoryAddress,
  dataSize: Long,
  rkey: RemoteKey
) extends RDMAOperation {
  def completionTime: Duration = {
    // RDMAå†™æ“ä½œçš„å»¶è¿Ÿæ¨¡å‹ï¼ˆé€šå¸¸æ¯”è¯»å¿«ï¼‰
    val baseLatency = 1.0 // å¾®ç§’çº§åŸºç¡€å»¶è¿Ÿ
    val bandwidthLatency = dataSize / networkBandwidth
    
    Duration.ofNanos((baseLatency + bandwidthLatency) * 1000)
  }
}

// RDMAçš„æ€§èƒ½æ¨¡å‹
class RDMAPerformanceModel {
  // ååé‡çš„ç†è®ºè®¡ç®—
  def theoreticalThroughput(
    operation: RDMAOperation,
    networkBandwidth: Bandwidth,
    pciBandwidth: Bandwidth
  ): Throughput = {
    // RDMAååé‡å—é™äºç½‘ç»œå’ŒPCIeçš„è¾ƒå°å€¼
    val effectiveBandwidth = math.min(networkBandwidth, pciBandwidth)
    Throughput(effectiveBandwidth * 0.9) // è€ƒè™‘åè®®å¼€é”€
  }
  
  // å»¶è¿Ÿçš„ç»„æˆåˆ†æ
  def latencyBreakdown(operation: RDMAOperation): LatencyBreakdown = {
    LatencyBreakdown(
      nicProcessing = 0.5,      // ç½‘å¡å¤„ç†æ—¶é—´
      networkTransit = 2.0,     // ç½‘ç»œä¼ è¾“æ—¶é—´
      remoteProcessing = 1.0,   // è¿œç«¯å¤„ç†æ—¶é—´
      memoryAccess = 0.8        // å†…å­˜è®¿é—®æ—¶é—´
    )
  }
  
  // é›¶æ‹·è´çš„æ•°å­¦ä¿è¯
  def zeroCopyGuarantee(operation: RDMAOperation): Boolean = {
    // é›¶æ‹·è´çš„æ¡ä»¶ï¼š
    // 1. æ•°æ®ç›´æ¥åœ¨ç½‘å¡å’Œå†…å­˜é—´ä¼ è¾“
    // 2. æ— éœ€CPUå‚ä¸æ•°æ®æ‹·è´
    // 3. å†…å­˜é¡µå¿…é¡»pinned
    isMemoryPinned(operation.sourceAddress) &&
    isMemoryPinned(operation.destinationAddress)
  }
}
```

#### ğŸ”„ é›¶æ‹·è´I/OæŠ€æœ¯çš„ç†è®ºåˆ†æ

**é›¶æ‹·è´æŠ€æœ¯çš„æ•°å­¦å»ºæ¨¡**:
```scala
// ä¼ ç»ŸI/O vs é›¶æ‹·è´I/Oçš„å¯¹æ¯”åˆ†æ
class ZeroCopyAnalysis {
  // ä¼ ç»ŸI/Oçš„æ•°æ®æ‹·è´æ¬¡æ•°
case class TraditionalIOCopyCount(
  userToKernel: Int = 1,      // ç”¨æˆ·ç©ºé—´åˆ°å†…æ ¸ç©ºé—´
  kernelToSocket: Int = 1,    // å†…æ ¸ç©ºé—´åˆ°socketç¼“å†²åŒº
  socketToNic: Int = 1        // socketç¼“å†²åŒºåˆ°ç½‘å¡
)
  
  // é›¶æ‹·è´I/Oçš„æ•°æ®æ‹·è´æ¬¡æ•°
case class ZeroCopyIOCopyCount(
  kernelToNic: Int = 1        // ç›´æ¥ä»å†…æ ¸ç©ºé—´åˆ°ç½‘å¡
)
  
  // I/Oæ€§èƒ½çš„ç†è®ºåˆ†æ
  def analyzeIOPerformance(
    dataSize: Long,
    memoryBandwidth: Bandwidth,
    copyOverhead: Double
  ): IOPerformanceAnalysis = {
    // ä¼ ç»ŸI/Oçš„æ€»å¼€é”€
    val traditionalCopies = 3 // ç”¨æˆ·->å†…æ ¸->socket->ç½‘å¡
    val traditionalOverhead = traditionalCopies * copyOverhead * dataSize
    
    // é›¶æ‹·è´I/Oçš„æ€»å¼€é”€
    val zeroCopyCopies = 1 // å†…æ ¸->ç½‘å¡
    val zeroCopyOverhead = zeroCopyCopies * copyOverhead * dataSize
    
    // æ€§èƒ½æå‡æ¯”
    val speedupRatio = traditionalOverhead / zeroCopyOverhead
    
    IOPerformanceAnalysis(
      traditionalIOTime = dataSize / memoryBandwidth + traditionalOverhead,
      zeroCopyIOTime = dataSize / memoryBandwidth + zeroCopyOverhead,
      speedupRatio = speedupRatio,
      bandwidthEfficiency = zeroCopyOverhead / traditionalOverhead
    )
  }
}

// é›¶æ‹·è´çš„å®ç°æŠ€æœ¯åˆ†æ
abstract class ZeroCopyTechnique {
  def name: String
  def applicability: Set[UseCase]
  def performanceGain: Double
  def complexity: ImplementationComplexity
}

case object SendFile extends ZeroCopyTechnique {
  val name = "sendfile() system call"
  val applicability = Set(FileTransfer)
  val performanceGain = 2.5 // 2.5xæ€§èƒ½æå‡
  val complexity = Medium
}

case object MemoryMapping extends ZeroCopyTechnique {
  val name = "memory mapping (mmap)"
  val applicability = Set(SharedMemory, FileTransfer)
  val performanceGain = 1.8 // 1.8xæ€§èƒ½æå‡
  val complexity = Low
}

case object DirectIO extends ZeroCopyTechnique {
  val name = "direct I/O (O_DIRECT)"
  val applicability = Set(Database, HighPerformanceStorage)
  val performanceGain = 1.5 // 1.5xæ€§èƒ½æå‡
  val complexity = High
}

// é›¶æ‹·è´çš„ç†è®ºæé™åˆ†æ
class ZeroCopyTheoreticalLimits {
  // Amdahlå®šå¾‹åœ¨é›¶æ‹·è´ä¸­çš„åº”ç”¨
  def amdahlSpeedup(
    parallelizableFraction: Double,
    processorCount: Int
  ): Double = {
    1.0 / (1.0 - parallelizableFraction + parallelizableFraction / processorCount)
  }
  
  // é›¶æ‹·è´çš„ç†è®ºä¸Šç•Œ
  def theoreticalUpperBound(
    networkLatency: Latency,
    processingLatency: Latency,
    memoryBandwidth: Bandwidth
  ): Duration = {
    // é›¶æ‹·è´çš„ç†è®ºæœ€å°å»¶è¿Ÿ = ç½‘ç»œå»¶è¿Ÿ + å¤„ç†å»¶è¿Ÿ
    // å†…å­˜å¸¦å®½æˆä¸ºå”¯ä¸€ç“¶é¢ˆ
    networkLatency + processingLatency
  }
  
  // å®é™…å®ç°ä¸ç†è®ºæé™çš„å·®è·åˆ†æ
  def implementationGap(
    actualPerformance: Performance,
    theoreticalLimit: Performance
  ): GapAnalysis = {
    val efficiency = actualPerformance.throughput / theoreticalLimit.throughput
    val overheadFactors = identifyOverheadFactors(actualPerformance, theoreticalLimit)
    
    GapAnalysis(
      efficiency = efficiency,
      overheadFactors = overheadFactors,
      optimizationPotential = 1.0 - efficiency
    )
  }
}
```

---

## ğŸ“ 4. å­¦ä¹ è·¯å¾„ä¸ç ”ç©¶æ–¹æ³•

### ğŸ“… 4å‘¨æ·±åº¦å­¦ä¹ è®¡åˆ’

#### ğŸ›ï¸ ç¬¬1å‘¨ï¼šç†è®ºåŸºç¡€æ„å»º
- **ç›®æ ‡**: æŒæ¡æ ¸å¿ƒæ•°å­¦æ¦‚å¿µå’Œç†è®ºæ¡†æ¶
- **é‡ç‚¹**: Actoræ¨¡å‹ã€CAPå®šç†ã€åŸºç¡€ç®—æ³•
- **è¾“å‡º**: ç†è®ºç¬”è®°ã€æ•°å­¦è¯æ˜ã€æ¦‚å¿µå›¾

#### ğŸ’» ç¬¬2å‘¨ï¼šç®—æ³•å®ç°æ·±å…¥
- **ç›®æ ‡**: å®ç°å…³é”®ç®—æ³•ï¼ŒéªŒè¯ç†è®ºæ­£ç¡®æ€§
- **é‡ç‚¹**: Paxos/Raftå®ç°ã€Volcanoä¼˜åŒ–å™¨åŸå‹
- **è¾“å‡º**: å¯è¿è¡Œä»£ç ã€æ€§èƒ½æµ‹è¯•ã€å®éªŒæŠ¥å‘Š

#### ğŸ§ª ç¬¬3å‘¨ï¼šç³»ç»Ÿè®¾è®¡å®è·µ
- **ç›®æ ‡**: è®¾è®¡å®Œæ•´çš„åˆ†å¸ƒå¼æŸ¥è¯¢å¤„ç†ç³»ç»Ÿ
- **é‡ç‚¹**: ç³»ç»Ÿæ¶æ„ã€æ€§èƒ½ä¼˜åŒ–ã€å®¹é”™è®¾è®¡
- **è¾“å‡º**: ç³»ç»Ÿè®¾è®¡æ–‡æ¡£ã€åŸå‹ç³»ç»Ÿã€æ€§èƒ½åˆ†æ

#### ğŸ”¬ ç¬¬4å‘¨ï¼šå‰æ²¿ç ”ç©¶æ¢ç´¢
- **ç›®æ ‡**: é˜…è¯»æœ€æ–°è®ºæ–‡ï¼Œè¯†åˆ«ç ”ç©¶æœºä¼š
- **é‡ç‚¹**: æœ€æ–°æŠ€æœ¯è¶‹åŠ¿ã€å¼€æ”¾æ€§é—®é¢˜ã€åˆ›æ–°æ–¹å‘
- **è¾“å‡º**: ç ”ç©¶ç»¼è¿°ã€åˆ›æ–°ææ¡ˆã€è®ºæ–‡è‰ç¨¿

### ğŸ“š æ¨èé˜…è¯»é¡ºåº

1. **åŸºç¡€ç†è®º** (1-2å‘¨)
   - Hewitt (1973) - Actoræ¨¡å‹å¥ åŸº
   - Brewer (2000) - CAPå®šç†
   - Lamport (1998) - Paxosç®—æ³•

2. **ç³»ç»Ÿå®ç°** (2-3å‘¨)
   - Graefe (1994) - Volcanoä¼˜åŒ–å™¨
   - Stonebraker (2005) - C-Storeè®¾è®¡
   - Ongaro (2014) - Raftç®—æ³•

3. **å‰æ²¿æŠ€æœ¯** (3-4å‘¨)
   - è¿‘5å¹´é¡¶çº§ä¼šè®®è®ºæ–‡
   - å¼€æºé¡¹ç›®æºç åˆ†æ
   - å·¥ä¸šç•ŒæŠ€æœ¯åšå®¢

### ğŸ”¬ ç ”ç©¶æ–¹æ³•è®º

#### ğŸ“Š ç†è®ºéªŒè¯æ–¹æ³•
- **æ•°å­¦è¯æ˜**: å½¢å¼åŒ–éªŒè¯ç®—æ³•æ­£ç¡®æ€§
- **æ€§èƒ½å»ºæ¨¡**: å»ºç«‹ç†è®ºæ€§èƒ½æ¨¡å‹
- **å®éªŒè®¾è®¡**: å¯¹ç…§å®éªŒéªŒè¯ç†è®ºå‡è®¾

#### ğŸ’» å®è·µéªŒè¯æ–¹æ³•
- **åŸå‹å®ç°**: å¿«é€ŸéªŒè¯ç†è®ºå¯è¡Œæ€§
- **åŸºå‡†æµ‹è¯•**: ç³»ç»Ÿæ€§èƒ½è¯„ä¼°
- **æ•…éšœæ³¨å…¥**: å®¹é”™æœºåˆ¶éªŒè¯

#### ğŸ“ˆ åˆ›æ–°ç ”ç©¶æ–¹æ³•
- **è·¨å­¦ç§‘æ€ç»´**: ç»“åˆæ•°å­¦ã€ç³»ç»Ÿã€ç½‘ç»œçŸ¥è¯†
- **é—®é¢˜é©±åŠ¨**: ä»å®é™…éœ€æ±‚å‡ºå‘å¯»æ‰¾åˆ›æ–°ç‚¹
- **å¼€æºè´¡çŒ®**: é€šè¿‡å¼€æºé¡¹ç›®éªŒè¯ç ”ç©¶æˆæœ

---

## ğŸ¯ æ€»ç»“

æœ¬ç†è®ºæ·±åº¦å­¦ä¹ æŒ‡å—ä¸ºä½ æä¾›äº†ï¼š

### ğŸ›ï¸ **ç†è®ºæ·±åº¦**
- ä»æ•°å­¦ç¬¬ä¸€æ€§åŸç†ç†è§£æ¯ä¸ªæ¦‚å¿µ
- æŒæ¡æ ¸å¿ƒç®—æ³•çš„å½¢å¼åŒ–å®šä¹‰å’Œè¯æ˜
- ç†è§£æŠ€æœ¯æ¼”è¿›çš„å†å²è„‰ç»œå’Œç†è®ºåŠ¨å› 

### ğŸ’» **å®è·µå¯¼å‘**
- æ¯ä¸ªç†è®ºéƒ½é…æœ‰å…·ä½“çš„ä»£ç å®ç°
- æä¾›å¯è¿è¡Œçš„å®éªŒéªŒè¯æ–¹æ³•
- ç»™å‡ºæ€§èƒ½ä¼˜åŒ–çš„å…·ä½“ç­–ç•¥

### ğŸ”¬ **ç ”ç©¶è§†é‡**
- è¯†åˆ«å¼€æ”¾æ€§ç ”ç©¶é—®é¢˜
- æä¾›å‰æ²¿æŠ€æœ¯å‘å±•è¶‹åŠ¿
- å»ºç«‹å­¦æœ¯ç ”ç©¶çš„æ€ç»´æ¡†æ¶

é€šè¿‡è¿™ä¸ªæŒ‡å—ï¼Œä½ å°†å»ºç«‹èµ·æ‰å®çš„ç†è®ºåŸºç¡€ï¼ŒåŸ¹å…»æ‰¹åˆ¤æ€§æ€ç»´ï¼Œä¸ºåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿå’Œå¤§æ•°æ®å¤„ç†é¢†åŸŸçš„æ·±å…¥ç ”ç©¶åšå¥½å‡†å¤‡ã€‚

> ğŸ’¡ **å­¦ä¹ å»ºè®®**: ç†è®ºä¸å®è·µå¹¶é‡ï¼Œåœ¨ç†è§£æ•°å­¦åŸç†çš„åŒæ—¶ï¼Œé€šè¿‡ä»£ç å®ç°åŠ æ·±ç†è§£ã€‚å®šæœŸæ€»ç»“å’Œåæ€ï¼Œå½¢æˆè‡ªå·±çš„çŸ¥è¯†ä½“ç³»ã€‚
  operator match {
    case Scan(table) => deriveScanProperties(table)
    case Filter(condition) => deriveFilterProperties(inputProperties.head)
    case Join(joinType) => deriveJoinProperties(inputProperties)
    case Aggregate(groupBy) => deriveAggregateProperties(inputProperties.head)
  }
}
```

---

### ğŸ’° 2.2 æˆæœ¬ä¼°ç®—æ¨¡å‹ç†è®º

#### ğŸ›ï¸ System RåŠ¨æ€è§„åˆ’ç®—æ³•

**ç»å…¸è®ºæ–‡**: "Access Path Selection in a Relational Database Management System" (Selinger et al., 1979)

**æˆæœ¬æ¨¡å‹**:
```scala
// System Ræˆæœ¬æ¨¡å‹
case class Cost(
  ioCost: Double,        // I/Oæˆæœ¬
  cpuCost: Double,       // CPUæˆæœ¬  
  networkCost: Double    // ç½‘ç»œæˆæœ¬
) {
  def total: Double = ioCost + cpuCost + networkCost
}

// é€‰æ‹©æ€§ä¼°ç®—
def selectivity(predicate: Predicate, statistics: TableStatistics): Double = {
  predicate match {
    case Equals(column, value) => 1.0 / statistics.columnDistinctCount(column)
    case Range(column, min, max) => (max - min) / statistics.columnRange(column)
    case Like(column, pattern) => 0.1  // å¯å‘å¼ä¼°ç®—
  }
}
```

**åŠ¨æ€è§„åˆ’ä¼˜åŒ–**:
```scala
// åŠ¨æ€è§„åˆ’ç®—æ³•å®ç°
def optimizeJoin(relation: Set[Relation]): Plan = {
  if (relation.size == 1) return ScanPlan(relation.head)
  
  val bestPlans = mutable.Map[Set[Relation], Plan]()
  
  for (size <- 2 to relation.size) {
    for (subset <- relation.subsets(size)) {
      val plans = for {
        (left, right) <- subset.split
        leftPlan = bestPlans(left)
        rightPlan = bestPlans(right)
        joinPlan = createJoinPlan(leftPlan, rightPlan)
      } yield joinPlan
      
      bestPlans(subset) = plans.minBy(_.cost)
    }
  }
  
  bestPlans(relation)
}
```

#### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ç†è®º

**ç›´æ–¹å›¾ä¼°ç®—**:
```scala
// ç­‰å®½ç›´æ–¹å›¾
case class EquiWidthHistogram(
  buckets: List[Bucket],
  minValue: Double,
  maxValue: Double
) {
  def estimateRangeQuery(min: Double, max: Double): Double = {
    val bucketWidth = (maxValue - minValue) / buckets.size
    val startBucket = ((min - minValue) / bucketWidth).toInt
    val endBucket = ((max - minValue) / bucketWidth).toInt
    
    if (startBucket == endBucket) {
      buckets(startBucket).frequency * (max - min) / bucketWidth
    } else {
      // è·¨å¤šä¸ªæ¡¶çš„ä¼°ç®—
      val fullBuckets = (endBucket - startBucket - 1).max(0)
      val partialBuckets = buckets(startBucket).frequency + buckets(endBucket).frequency
      fullBuckets + partialBuckets
    }
  }
}
```

---

### ğŸš€ 2.3 å‘é‡åŒ–æ‰§è¡Œç†è®º

#### ğŸ’» SIMDæŒ‡ä»¤ç†è®ºåŸºç¡€

**å‘é‡åŒ–åŸç†**:
```scala
// æ ‡é‡æ‰§è¡Œ vs å‘é‡åŒ–æ‰§è¡Œ
// æ ‡é‡æ‰§è¡Œ
def scalarSum(array: Array[Int]): Int = {
  var sum = 0
  for (i <- 0 until array.length) {
    sum += array(i)
  }
  sum
}

// å‘é‡åŒ–æ‰§è¡Œ (æ¦‚å¿µ)
def vectorizedSum(array: Array[Int]): Int = {
  // ä½¿ç”¨SIMDæŒ‡ä»¤ä¸€æ¬¡å¤„ç†å¤šä¸ªå…ƒç´ 
  // ä¼ªä»£ç : sum = SIMD_ADD(array[0:3], array[4:7], ...)
}
```

**å‘é‡åŒ–æ‰§è¡Œå¼•æ“**:
```scala
// å‘é‡åŒ–æ‰§è¡Œç®—å­
trait VectorizedOperator {
  def execute(input: VectorizedBatch): VectorizedBatch
}

case class VectorizedBatch(
  vectors: Map[String, ArrowVector],
  rowCount: Int
) {
  def select(predicate: ArrowVector): VectorizedBatch = {
    // ä½¿ç”¨SIMDæŒ‡ä»¤è¿›è¡Œè°“è¯è¯„ä¼°
    val selectionVector = evaluatePredicate(predicate)
    filterBySelectionVector(selectionVector)
  }
}
```

#### ğŸ¯ Apache Arrowå‘é‡åŒ–æ¶æ„

**Arrowå†…å­˜å¸ƒå±€**:
```scala
// Arrowæ•°ç»„çš„å†…å­˜å¸ƒå±€
case class ArrowArray(
  buffers: List[ByteBuffer],  // ç¼“å†²åŒºåˆ—è¡¨
  length: Int,                // å…ƒç´ ä¸ªæ•°
  nullCount: Int,             // ç©ºå€¼ä¸ªæ•°
  offset: Int                 // åç§»é‡
)

// ä¾‹å¦‚Int32Arrayçš„å¸ƒå±€
// Buffer 0: Validity bitmap (ç©ºå€¼ä½å›¾)
// Buffer 1: Data buffer (å®é™…æ•°æ®)
```

**é›¶æ‹·è´ä¼ è¾“**:
```scala
// Arrowçš„é›¶æ‹·è´åºåˆ—åŒ–
def serializeBatch(batch: ArrowBatch): ByteBuffer = {
  // ç›´æ¥è¿”å›å†…å­˜æ˜ å°„ï¼Œæ— éœ€å¤åˆ¶
  batch.getUnderlyingBuffer()
}

// é›¶æ‹·è´ååºåˆ—åŒ–
def deserializeBatch(buffer: ByteBuffer): ArrowBatch = {
  // ç›´æ¥ä½¿ç”¨å†…å­˜æ˜ å°„ï¼Œæ— éœ€è§£æ
  ArrowBatch.fromBuffer(buffer)
}
```

---

## ğŸ§  3. å†…å­˜è®¡ç®—ä¸é›¶æ‹·è´ç†è®º

### ğŸ“Š 3.1 åˆ—å¼å­˜å‚¨ç†è®º

#### ğŸ›ï¸ C-Storeè®¾è®¡åŸç†

**ç»å…¸è®ºæ–‡**: "C-Store: A Column-oriented DBMS" (Stonebraker et al., 2005)

**åˆ—å¼å­˜å‚¨çš„ä¼˜åŠ¿**:
```
è¡Œå¼å­˜å‚¨: [id, name, age], [id, name, age], [id, name, age]
åˆ—å¼å­˜å‚¨: [id, id, id], [name, name, name], [age, age, age]

æŸ¥è¯¢ä¼˜åŠ¿:
- åªè¯»å–éœ€è¦çš„åˆ—ï¼Œå‡å°‘I/O
- æ›´å¥½çš„å‹ç¼©æ¯”
- ç¼“å­˜å±€éƒ¨æ€§æ›´å¥½
```

**å‹ç¼©ç®—æ³•ç†è®º**:
```scala
// åˆ—å¼å­˜å‚¨å‹ç¼©ç­–ç•¥
sealed trait CompressionStrategy
case object RunLengthEncoding extends CompressionStrategy  // æ¸¸ç¨‹ç¼–ç 
case object DictionaryEncoding extends CompressionStrategy  // å­—å…¸ç¼–ç 
case object DeltaEncoding extends CompressionStrategy       // å¢é‡ç¼–ç 
case object BitPacking extends CompressionStrategy          // ä½æ‰“åŒ…

// å‹ç¼©é€‰æ‹©ç®—æ³•
def selectCompression(column: Column): CompressionStrategy = {
  val cardinality = column.distinctCount
  val sortedness = column.sortedness
  
  if (cardinality < column.length * 0.1) DictionaryEncoding
  else if (sortedness > 0.8) DeltaEncoding  
  else if (column.isNumeric) BitPacking
  else RunLengthEncoding
}
```

#### ğŸš€ MonetDBå‘é‡åŒ–æ‰§è¡Œ

**æ ¸å¿ƒè®ºæ–‡**: "MonetDB/X100: Pushing the Limits of SQL Main Memory Databases" (Zukowski et al., 2006)

**å‘é‡åŒ–æ‰§è¡Œæ¨¡å‹**:
```scala
// å‘é‡åŒ–æ‰§è¡Œå¼•æ“
case class MALPlan(
  operators: List[MALOperator],
  batchSizes: List[Int]
)

// MALæ“ä½œç¬¦ç¤ºä¾‹
case class AlgebraicSelect(
  predicate: Expression,
  input: BAT[_, _]          // Binary Association Table
) extends MALOperator {
  def execute(): BAT[_, _] = {
    // ä½¿ç”¨SIMDæŒ‡ä»¤è¿›è¡Œæ‰¹é‡é€‰æ‹©
    input.selectVectorized(predicate)
  }
}
```

---

### ğŸ—ï¸ 3.2 NUMAæ¶æ„ä¼˜åŒ–

#### ğŸ§  NUMAç†è®ºåŸºç¡€

**NUMAæ¶æ„ç‰¹ç‚¹**:
```
Uniform Memory Access (UMA):
æ‰€æœ‰CPUè®¿é—®å†…å­˜çš„é€Ÿåº¦ç›¸åŒ

Non-Uniform Memory Access (NUMA):
CPUè®¿é—®æœ¬åœ°å†…å­˜é€Ÿåº¦å¿«ï¼Œè®¿é—®è¿œç¨‹å†…å­˜é€Ÿåº¦æ…¢
```

**å†…å­˜è®¿é—®å»¶è¿Ÿæ¨¡å‹**:
```scala
// NUMAå»¶è¿Ÿæ¨¡å‹
case class NUMATopology(
  nodes: List[NUMANode],
  distances: Map[(Int, Int), Int]  // èŠ‚ç‚¹é—´è·ç¦»çŸ©é˜µ
)

case class NUMANode(
  nodeId: Int,
  cpus: List[Int],
  memorySize: Long,
  localLatency: Int,
  remoteLatency: Map[Int, Int]
)

// å†…å­˜åˆ†é…ç­–ç•¥
def allocateMemory(size: Long, preferredNode: Int): MemoryBlock = {
  if (NUMANode(preferredNode).hasEnoughMemory(size)) {
    allocateLocal(preferredNode, size)
  } else {
    // è·¨èŠ‚ç‚¹åˆ†é…
    allocateInterleaved(size)
  }
}
```

#### ğŸ“Š æ•°æ®å±€éƒ¨æ€§ä¼˜åŒ–

**æ•°æ®æ”¾ç½®ç­–ç•¥**:
```scala
// æ•°æ®å±€éƒ¨æ€§ä¼˜åŒ–ç®—æ³•
def optimizeDataPlacement(
  data: List[DataBlock], 
  accessPattern: AccessPattern
): PlacementStrategy = {
  
  val affinityGraph = buildAffinityGraph(data, accessPattern)
  val partitioning = graphPartitioning(affinityGraph, NUMANode.count)
  
  PlacementStrategy(partitioning)
}

// è®¿é—®æ¨¡å¼åˆ†æ
case class AccessPattern(
  hotData: Set[DataBlock],
  accessFrequency: Map[DataBlock, Double],
  accessLocality: Map[(DataBlock, DataBlock), Double]
)
```

---

### ğŸš€ 3.3 é«˜æ•ˆæ•°æ®ä¼ è¾“ç†è®º

#### ğŸŒ RDMAæŠ€æœ¯åŸç†

**RDMA vs ä¼ ç»Ÿç½‘ç»œI/O**:
```
ä¼ ç»Ÿç½‘ç»œI/O:
CPU -> å†…å­˜æ‹·è´ -> å†…æ ¸ç©ºé—´ -> ç½‘ç»œå¡ -> ç½‘ç»œ

RDMA (Remote Direct Memory Access):
åº”ç”¨ -> RDMAç½‘å¡ -> è¿œç¨‹å†…å­˜ (é›¶æ‹·è´)
```

**RDMAæ“ä½œç±»å‹**:
```scala
// RDMAæ“ä½œå®šä¹‰
sealed trait RDMOperation
case class Send(queuePair: QueuePair, buffer: ByteBuffer) extends RDMOperation
case class Recv(queuePair: QueuePair, buffer: ByteBuffer) extends RDMOperation
case class Write(remoteAddr: RemoteAddress, buffer: ByteBuffer) extends RDMOperation
case class Read(remoteAddr: RemoteAddress, buffer: ByteBuffer) extends RDMOperation

// RDMAé˜Ÿåˆ—å¯¹
case class QueuePair(
  sendQueue: CompletionQueue,
  recvQueue: CompletionQueue,
  state: QueuePairState
)
```

#### ğŸ”„ é›¶æ‹·è´I/OæŠ€æœ¯

**é›¶æ‹·è´æŠ€æœ¯åˆ†ç±»**:
```scala
// é›¶æ‹·è´å®ç°æ–¹å¼
sealed trait ZeroCopyTechnique
case object MMap extends ZeroCopyTechnique           // å†…å­˜æ˜ å°„
case object SendFile extends ZeroCopyTechnique        // sendfileç³»ç»Ÿè°ƒç”¨
case object DirectBuffer extends ZeroCopyTechnique    // ç›´æ¥ç¼“å†²åŒº
case object SharedMemory extends ZeroCopyTechnique    // å…±äº«å†…å­˜

// é›¶æ‹·è´æ–‡ä»¶ä¼ è¾“
def zeroCopyTransfer(input: FileChannel, output: SocketChannel): Long = {
  input.transferTo(0, input.size(), output)
}
```

**å†…å­˜æ˜ å°„I/O**:
```scala
// å†…å­˜æ˜ å°„çš„å®ç°
case class MemoryMappedFile(
  file: RandomAccessFile,
  buffer: MappedByteBuffer,
  size: Long
) {
  def read(position: Long, length: Int): ByteBuffer = {
    buffer.position(position.toInt)
    buffer.slice().limit(length)
  }
  
  def write(position: Long, data: ByteBuffer): Unit = {
    buffer.position(position.toInt)
    buffer.put(data)
  }
}
```

---

## ğŸ¯ å­¦ä¹ è·¯å¾„å»ºè®®

### ğŸ“… ç¬¬ä¸€å‘¨ï¼šç†è®ºåŸºç¡€
1. **Actoræ¨¡å‹**: é˜…è¯»Hewitt 1973å¹´è®ºæ–‡ï¼Œç†è§£åŸºæœ¬æ¦‚å¿µ
2. **ä¸€è‡´æ€§ç†è®º**: å­¦ä¹ CAPå®šç†å’ŒFLPä¸å¯èƒ½æ€§
3. **æŸ¥è¯¢ä¼˜åŒ–**: ç ”ç©¶Volcanoè®ºæ–‡ï¼Œç†è§£ä¼˜åŒ–å™¨æ¡†æ¶

### ğŸ“… ç¬¬äºŒå‘¨ï¼šç®—æ³•æ·±å…¥
1. **å…±è¯†ç®—æ³•**: å®ç°Paxoså’ŒRaftçš„æ ¸å¿ƒé€»è¾‘
2. **æˆæœ¬ä¼°ç®—**: å®ç°System Rçš„åŠ¨æ€è§„åˆ’ç®—æ³•
3. **å‘é‡åŒ–æ‰§è¡Œ**: ç†è§£SIMDæŒ‡ä»¤å’ŒArrowå†…å­˜å¸ƒå±€

### ğŸ“… ç¬¬ä¸‰å‘¨ï¼šå®è·µéªŒè¯
1. **Actorç³»ç»Ÿ**: å®ç°ç®€å•çš„åˆ†å¸ƒå¼Actoræ¡†æ¶
2. **æŸ¥è¯¢ä¼˜åŒ–å™¨**: æ„å»ºåŸºç¡€çš„SQLä¼˜åŒ–å™¨
3. **å†…å­˜ç®¡ç†**: å®ç°é›¶æ‹·è´æ•°æ®ä¼ è¾“

### ğŸ“… ç¬¬å››å‘¨ï¼šå‰æ²¿æ¢ç´¢
1. **æœ€æ–°è®ºæ–‡**: é˜…è¯»VLDB/SIGMODæœ€æ–°è®ºæ–‡
2. **å®éªŒè®¾è®¡**: è®¾è®¡æ€§èƒ½å¯¹æ¯”å®éªŒ
3. **åˆ›æ–°æ€è€ƒ**: æå‡ºè‡ªå·±çš„æ”¹è¿›æ–¹æ¡ˆ

---

## ğŸ“š æ¨èè®ºæ–‡æ¸…å•

### ğŸ† å¿…è¯»ç»å…¸
1. **"A Universal Modular Actor Formalism for Artificial Intelligence"** (Hewitt et al., 1973)
2. **"The Volcano Optimizer Generator"** (Graefe, 1994)
3. **"Access Path Selection in a Relational Database Management System"** (Selinger et al., 1979)
4. **"C-Store: A Column-oriented DBMS"** (Stonebraker et al., 2005)
5. **"MonetDB/X100: Pushing the Limits"** (Zukowski et al., 2006)

### ğŸ”¬ å‰æ²¿ç ”ç©¶
1. **"The Apache Arrow Columnar In-Memory Analytics System"** (2016-2023)
2. **"DataFusion: A Query Engine for Apache Arrow"** (2022)
3. **"Learning-based Query Optimization"** (Marcus & Papaemmanouil, 2019)
4. **"Adaptive Query Processing: A Survey"** (Ioannidis, 2002)

---

## ğŸ”¬ å®éªŒéªŒè¯å»ºè®®

### ğŸ§ª ç†è®ºéªŒè¯å®éªŒ
1. **Actoræ¨¡å‹**: å®ç°æ¶ˆæ¯ä¼ é€’çš„å¯é æ€§éªŒè¯
2. **ä¸€è‡´æ€§ç®—æ³•**: å¯¹æ¯”Paxos/Raftçš„æ€§èƒ½å’Œæ­£ç¡®æ€§
3. **æŸ¥è¯¢ä¼˜åŒ–**: éªŒè¯æˆæœ¬ä¼°ç®—æ¨¡å‹çš„å‡†ç¡®æ€§
4. **å‘é‡åŒ–æ‰§è¡Œ**: å¯¹æ¯”æ ‡é‡å’Œå‘é‡åŒ–çš„æ€§èƒ½å·®å¼‚

### ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•
1. **TPC-DS**: å†³ç­–æ”¯æŒç³»ç»ŸåŸºå‡†
2. **YCSB**: äº‘æœåŠ¡åŸºå‡†
3. **è‡ªå®šä¹‰å¾®åŸºå‡†**: é’ˆå¯¹ç‰¹å®šç®—æ³•çš„æµ‹è¯•

---

> ğŸ’¡ **å­¦ä¹ å»ºè®®**: å°†ç†è®ºå­¦ä¹ ä¸å®è·µå®ç°ç›¸ç»“åˆï¼Œæ¯ä¸ªç†è®ºæ¦‚å¿µéƒ½è¦é€šè¿‡ä»£ç æ¥éªŒè¯ç†è§£ç¨‹åº¦ã€‚åŒæ—¶å…³æ³¨æœ€æ–°ç ”ç©¶è¿›å±•ï¼ŒåŸ¹å…»æ‰¹åˆ¤æ€§æ€ç»´å’Œåˆ›æ–°èƒ½åŠ›ã€‚
