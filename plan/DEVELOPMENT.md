# ğŸ› ï¸ å¼€å‘æŒ‡å—

## å¼€å‘ç¯å¢ƒè®¾ç½®

### å¿…éœ€å·¥å…·

- JDK 11+
- Scala 2.13.12
- SBT 1.9.8+
- Git
- IDE (æ¨è IntelliJ IDEA + Scala Plugin)

### IDEé…ç½®

#### IntelliJ IDEA

1. å®‰è£…Scalaæ’ä»¶
2. å¯¼å…¥é¡¹ç›®ï¼šFile â†’ Open â†’ é€‰æ‹©build.sbt
3. ç­‰å¾…SBTä¸‹è½½ä¾èµ–
4. é…ç½®JDK 11

#### VS Code

1. å®‰è£…Metalsæ’ä»¶
2. å®‰è£…Scala Syntaxæ’ä»¶
3. æ‰“å¼€é¡¹ç›®æ–‡ä»¶å¤¹
4. Metalsä¼šè‡ªåŠ¨é…ç½®é¡¹ç›®

## é¡¹ç›®ç»“æ„è¯¦è§£

### æ ¸å¿ƒæ¨¡å—

```
pekko-server/src/main/scala/cn/xuyinyin/magic/
â”œâ”€â”€ core/                     # æ ¸å¿ƒç³»ç»Ÿ
â”‚   â”œâ”€â”€ cluster/              # é›†ç¾¤ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ PekkoGuardian.scala      # å…¨å±€å®ˆæŠ¤è€…Actor
â”‚   â”‚   â”œâ”€â”€ ClusterListener.scala    # é›†ç¾¤äº‹ä»¶ç›‘å¬
â”‚   â”‚   â”œâ”€â”€ HealthChecker.scala      # å¥åº·æ£€æŸ¥Actor
â”‚   â”‚   â””â”€â”€ NodeRole.scala           # èŠ‚ç‚¹è§’è‰²å®šä¹‰
â”‚   â””â”€â”€ config/               # é…ç½®ç®¡ç†
â”‚       â””â”€â”€ ConfigLoader.scala
â”‚
â”œâ”€â”€ workflow/                 # å·¥ä½œæµç³»ç»Ÿ â­â­â­
â”‚   â”œâ”€â”€ model/                # æ•°æ®æ¨¡å‹
â”‚   â”‚   â””â”€â”€ WorkflowDSL.scala        # DSLå®šä¹‰
â”‚   â”œâ”€â”€ actors/               # Actorç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ WorkflowSupervisor.scala # å·¥ä½œæµç›‘ç£è€…
â”‚   â”‚   â””â”€â”€ WorkflowActor.scala      # å·¥ä½œæµActor
â”‚   â”œâ”€â”€ engine/               # æ‰§è¡Œå¼•æ“
â”‚   â”‚   â”œâ”€â”€ WorkflowExecutionEngine.scala
â”‚   â”‚   â”œâ”€â”€ executors/        # æ‰§è¡Œå™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ NodeExecutor.scala
â”‚   â”‚   â”‚   â”œâ”€â”€ SourceExecutor.scala
â”‚   â”‚   â”‚   â”œâ”€â”€ TransformExecutor.scala
â”‚   â”‚   â”‚   â””â”€â”€ SinkExecutor.scala
â”‚   â”‚   â””â”€â”€ registry/         # æ³¨å†Œä¸­å¿ƒ
â”‚   â”‚       â””â”€â”€ NodeRegistry.scala
â”‚   â”œâ”€â”€ nodes/                # èŠ‚ç‚¹å®ç°
â”‚   â”‚   â”œâ”€â”€ base/             # åŸºç¡€å®šä¹‰
â”‚   â”‚   â”‚   â”œâ”€â”€ NodeSource.scala
â”‚   â”‚   â”‚   â””â”€â”€ NodeSink.scala
â”‚   â”‚   â”œâ”€â”€ sources/          # SourceèŠ‚ç‚¹
â”‚   â”‚   â”‚   â”œâ”€â”€ MySQLSource.scala
â”‚   â”‚   â”‚   â””â”€â”€ FileSource.scala
â”‚   â”‚   â”œâ”€â”€ transforms/       # TransformèŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ sinks/            # SinkèŠ‚ç‚¹
â”‚   â”‚       â””â”€â”€ MySQLSink.scala
â”‚   â””â”€â”€ scheduler/            # è°ƒåº¦ç³»ç»Ÿ
â”‚       â”œâ”€â”€ WorkflowScheduler.scala
â”‚       â””â”€â”€ SchedulerManager.scala
â”‚
â””â”€â”€ api/                      # APIæ¥å£
    â””â”€â”€ http/
        â”œâ”€â”€ models/           # æ•°æ®æ¨¡å‹
        â””â”€â”€ routes/           # è·¯ç”±
            â”œâ”€â”€ HttpRoutes.scala
            â””â”€â”€ WorkflowRoutes.scala
```

## æ·»åŠ æ–°èŠ‚ç‚¹

### æ­¥éª¤1ï¼šåˆ›å»ºèŠ‚ç‚¹ç±»

#### SourceèŠ‚ç‚¹ç¤ºä¾‹

```scala
package cn.xuyinyin.magic.workflow.nodes.sources

import cn.xuyinyin.magic.workflow.model.WorkflowDSL
import cn.xuyinyin.magic.workflow.nodes.base.NodeSource
import org.apache.pekko.NotUsed
import org.apache.pekko.stream.scaladsl.Source
import spray.json._

class HTTPSource extends NodeSource {
  
  override def createSource(
    node: WorkflowDSL.Node,
    onLog: String => Unit
  ): Source[String, NotUsed] = {
    
    val url = node.config.getOrElse("url", "").toString
    val method = node.config.getOrElse("method", "GET").toString
    
    onLog(s"HTTPè¯·æ±‚: $method $url")
    
    // å®ç°HTTPè¯·æ±‚é€»è¾‘
    Source.single(s"""{"data": "from $url"}""")
  }
}
```

#### TransformèŠ‚ç‚¹ç¤ºä¾‹

```scala
package cn.xuyinyin.magic.workflow.nodes.transforms

import org.apache.pekko.NotUsed
import org.apache.pekko.stream.scaladsl.Flow

class JSONParser {
  
  def createTransform(
    node: WorkflowDSL.Node,
    onLog: String => Unit
  ): Flow[String, String, NotUsed] = {
    
    onLog("JSONè§£æè½¬æ¢")
    
    Flow[String].map { jsonStr =>
      // JSONè§£æé€»è¾‘
      jsonStr.parseJson.prettyPrint
    }
  }
}
```

#### SinkèŠ‚ç‚¹ç¤ºä¾‹

```scala
package cn.xuyinyin.magic.workflow.nodes.sinks

import cn.xuyinyin.magic.workflow.model.WorkflowDSL
import cn.xuyinyin.magic.workflow.nodes.base.NodeSink
import org.apache.pekko.Done
import org.apache.pekko.stream.scaladsl.Sink
import scala.concurrent.{ExecutionContext, Future}

class ElasticsearchSink extends NodeSink {
  
  override def createSink(
    node: WorkflowDSL.Node,
    onLog: String => Unit
  )(implicit ec: ExecutionContext): Sink[String, Future[Done]] = {
    
    val index = node.config.getOrElse("index", "default").toString
    
    onLog(s"å†™å…¥Elasticsearchç´¢å¼•: $index")
    
    Sink.foreach[String] { data =>
      // å†™å…¥ESé€»è¾‘
      println(s"Indexing: $data")
    }
  }
}
```

### æ­¥éª¤2ï¼šæ³¨å†ŒèŠ‚ç‚¹

åœ¨`NodeRegistry.scala`ä¸­æ³¨å†Œï¼š

```scala
class NodeRegistry(implicit ec: ExecutionContext) {
  
  private val sources: Map[String, NodeSource] = Map(
    "random.numbers" -> new RandomNumbersSource(),
    "file.csv" -> new FileSource(),
    "http.request" -> new HTTPSource()  // â¬…ï¸ æ–°å¢
  )
  
  private val sinks: Map[String, NodeSink] = Map(
    "console.log" -> new ConsoleLogSink(),
    "file.text" -> new FileTextSink(),
    "elasticsearch" -> new ElasticsearchSink()  // â¬…ï¸ æ–°å¢
  )
}
```

### æ­¥éª¤3ï¼šæ·»åŠ åˆ°Executor

åœ¨`SourceExecutor.scala`ä¸­æ·»åŠ æ”¯æŒï¼š

```scala
class SourceExecutor extends NodeExecutor {
  
  override def supportedTypes: Set[String] = Set(
    "file.csv",
    "random.numbers",
    "http.request",  // â¬…ï¸ æ–°å¢
    // ... å…¶ä»–ç±»å‹
  )
}
```

### æ­¥éª¤4ï¼šæ›´æ–°å‰ç«¯

åœ¨å‰ç«¯èŠ‚ç‚¹é¢æ¿æ·»åŠ æ–°èŠ‚ç‚¹ï¼š

```typescript
// xxt-ui/src/config/nodeTypes.ts
export const nodeTypes = {
  sources: [
    { type: 'random.numbers', label: 'éšæœºæ•°ç”Ÿæˆ' },
    { type: 'file.csv', label: 'CSVæ–‡ä»¶' },
    { type: 'http.request', label: 'HTTPè¯·æ±‚' }, // â¬…ï¸ æ–°å¢
  ],
  // ...
};
```

## ç¼–è¯‘å’Œæµ‹è¯•

### ç¼–è¯‘é¡¹ç›®

```bash
# æ¸…ç†
sbt clean

# ç¼–è¯‘
sbt "project pekko-server" compile

# è¿è¡Œæµ‹è¯•
sbt "project pekko-server" test
```

### å•å…ƒæµ‹è¯•

```scala
class WorkflowExecutionEngineSpec extends AnyFlatSpec {
  
  "WorkflowExecutionEngine" should "validate workflow correctly" in {
    val workflow = WorkflowDSL.Workflow(
      id = "test_1",
      name = "æµ‹è¯•å·¥ä½œæµ",
      nodes = List(
        Node("s1", "random.numbers", "source", Map()),
        Node("k1", "console.log", "sink", Map())
      ),
      edges = List(Edge("s1", "k1"))
    )
    
    // æµ‹è¯•éªŒè¯é€»è¾‘
  }
}
```

### é›†æˆæµ‹è¯•

```bash
# å¯åŠ¨æœåŠ¡
sbt "project pekko-server" run &

# è¿è¡Œé›†æˆæµ‹è¯•
curl -X POST http://localhost:8080/api/v1/workflows -d @test-workflow.json

# åœæ­¢æœåŠ¡
kill %1
```

## è°ƒè¯•æŠ€å·§

### å¯ç”¨è°ƒè¯•æ—¥å¿—

åœ¨`logback.xml`ä¸­ï¼š

```xml
<logger name="cn.xuyinyin.magic.workflow" level="DEBUG"/>
```

### è¿œç¨‹è°ƒè¯•

```bash
# å¯åŠ¨æ—¶æ·»åŠ è°ƒè¯•å‚æ•°
sbt -jvm-debug 5005 "project pekko-server" run
```

ç„¶ååœ¨IDEä¸­è¿æ¥åˆ°ç«¯å£5005ã€‚

### Actoræ¶ˆæ¯è·Ÿè¸ª

```scala
import org.apache.pekko.actor.typed.receptionist.Receptionist

// è®¢é˜…Actorç³»ç»Ÿäº‹ä»¶
context.system.receptionist ! Receptionist.Subscribe(...)
```

## ä»£ç è§„èŒƒ

### Scala Style

éµå¾ªScalaå®˜æ–¹ç¼–ç è§„èŒƒï¼š

```scala
// å¥½çš„
def processData(input: String): Future[Result] = {
  Future.successful(Result(input))
}

// é¿å…
def processData(input:String):Future[Result]={Future.successful(Result(input))}
```

### å‘½åçº¦å®š

- ç±»åï¼šPascalCaseï¼ˆå¦‚`WorkflowActor`ï¼‰
- æ–¹æ³•åï¼šcamelCaseï¼ˆå¦‚`createWorkflow`ï¼‰
- å¸¸é‡ï¼šUPPER_SNAKE_CASEï¼ˆå¦‚`MAX_RETRIES`ï¼‰
- åŒ…åï¼šå°å†™ï¼ˆå¦‚`workflow.actors`ï¼‰

### æ–‡æ¡£æ³¨é‡Š

```scala
/**
 * å·¥ä½œæµæ‰§è¡Œå¼•æ“
 * 
 * åŸºäºPekko Streamå®ç°DSLæ‰§è¡Œ
 * 
 * @param system Actorç³»ç»Ÿ
 * @param ec æ‰§è¡Œä¸Šä¸‹æ–‡
 */
class WorkflowExecutionEngine(
  implicit system: ActorSystem[_],
  ec: ExecutionContext
) {
  // ...
}
```

## æ€§èƒ½ä¼˜åŒ–

### 1. é¿å…é˜»å¡æ“ä½œ

```scala
// âŒ é”™è¯¯ - é˜»å¡
val result = Await.result(future, Duration.Inf)

// âœ… æ­£ç¡® - å¼‚æ­¥
future.map { result =>
  // å¤„ç†ç»“æœ
}
```

### 2. åˆç†ä½¿ç”¨èƒŒå‹

```scala
source
  .buffer(100, OverflowStrategy.backpressure)
  .via(transform)
  .to(sink)
```

### 3. æ‰¹é‡å¤„ç†

```scala
source
  .grouped(100)  // æ‰¹é‡å¤„ç†
  .mapAsync(4) { batch =>
    // å¼‚æ­¥å¤„ç†æ‰¹æ¬¡
  }
  .to(sink)
```

## å¸¸è§é—®é¢˜

### ç¼–è¯‘é”™è¯¯

**é—®é¢˜**: æ‰¾ä¸åˆ°æŸä¸ªåŒ…  
**è§£å†³**: 
```bash
sbt clean
sbt update
sbt compile
```

### Actoræ— å“åº”

**é—®é¢˜**: Actoræ¶ˆæ¯æ²¡æœ‰å“åº”  
**è§£å†³**: æ£€æŸ¥Actorçš„è¡Œä¸ºå®šä¹‰å’Œæ¶ˆæ¯å¤„ç†

```scala
def behavior: Behavior[Command] = Behaviors.receive { (context, message) =>
  context.log.debug(s"Received: $message")  // æ·»åŠ æ—¥å¿—
  // å¤„ç†æ¶ˆæ¯
  Behaviors.same
}
```

### å†…å­˜æº¢å‡º

**é—®é¢˜**: OutOfMemoryError  
**è§£å†³**: å¢åŠ JVMå†…å­˜

```bash
sbt -J-Xmx4G "project pekko-server" run
```

## è´¡çŒ®ä»£ç 

### æäº¤å‰æ£€æŸ¥æ¸…å•

- [ ] ä»£ç é€šè¿‡ç¼–è¯‘
- [ ] æ·»åŠ äº†å•å…ƒæµ‹è¯•
- [ ] æ›´æ–°äº†æ–‡æ¡£
- [ ] éµå¾ªä»£ç è§„èŒƒ
- [ ] æäº¤ä¿¡æ¯æ¸…æ™°

### Gitå·¥ä½œæµ

```bash
# 1. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
git checkout -b feature/new-node-type

# 2. å¼€å‘å’Œæäº¤
git add .
git commit -m "feat: æ·»åŠ HTTPè¯·æ±‚èŠ‚ç‚¹"

# 3. æ¨é€åˆ°è¿œç¨‹
git push origin feature/new-node-type

# 4. åˆ›å»ºPull Request
```

### Commitæ¶ˆæ¯è§„èŒƒ

```
feat: æ·»åŠ æ–°åŠŸèƒ½
fix: ä¿®å¤bug
docs: æ–‡æ¡£æ›´æ–°
style: ä»£ç æ ¼å¼è°ƒæ•´
refactor: ä»£ç é‡æ„
test: æµ‹è¯•ç›¸å…³
chore: æ„å»º/å·¥å…·ç›¸å…³
```

## ç›¸å…³æ–‡æ¡£

- [å¿«é€Ÿå¼€å§‹](QUICKSTART.md)
- [APIä½¿ç”¨](API_USAGE.md)
- [æ·»åŠ æ–°èŠ‚ç‚¹æŒ‡å—](../ADD_NEW_NODE_GUIDE.md)
- [Week2è¿›åº¦](../WEEK2_PROGRESS.md)
